{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../nn_survival_analysis')\n",
    "from nn_survival_analysis.general_utils import *\n",
    "from nn_survival_analysis.model_utils import *\n",
    "from nn_survival_analysis.losses import *\n",
    "from nn_survival_analysis.models import *\n",
    "from nn_survival_analysis.other_nn_models import *\n",
    "from nn_survival_analysis.time_invariant_surv import *\n",
    "from nn_survival_analysis.time_variant_surv import *\n",
    "from nn_survival_analysis.traditional_models import *\n",
    "import scipy\n",
    "from pycox.datasets import support\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from pycox.preprocessing.feature_transforms import OrderedCategoricalLong\n",
    "\n",
    "# define sigmoid function - will be handy later\n",
    "sigmoid = lambda z : 1 / (1 + np.exp(-z))\n",
    "\n",
    "config_file_path = '../nn_survival_analysis/config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'support' not locally available. Downloading...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Get configs\n",
    "with open(config_file_path , \"r\") as file:\n",
    "    configs = json.load(file)\n",
    "\n",
    "df_train = support.read_df()\n",
    "df_test = df_train.sample(frac=0.2)\n",
    "df_train = df_train.drop(df_test.index)\n",
    "df_val = df_train.sample(frac=0.2)\n",
    "df_train = df_train.drop(df_val.index)\n",
    "\n",
    "cols_standardize =  ['x0', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13']\n",
    "cols_leave = ['x1', 'x4', 'x5']\n",
    "cols_categorical =  ['x2', 'x3', 'x6']\n",
    "\n",
    "standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "leave = [(col, None) for col in cols_leave]\n",
    "categorical = [(col, OrderedCategoricalLong()) for col in cols_categorical]\n",
    "\n",
    "x_mapper = DataFrameMapper(standardize + leave + categorical)\n",
    "\n",
    "x_train = x_mapper.fit_transform(df_train).astype('float32')\n",
    "x_val = x_mapper.transform(df_val).astype('float32')\n",
    "x_test = x_mapper.transform(df_test).astype('float32')\n",
    "\n",
    "get_target = lambda df: (df['duration'].values, df['event'].values)\n",
    "y_train_dur , y_train_event = get_target(df_train)\n",
    "y_test_dur , y_test_event = get_target(df_test)\n",
    "y_val_dur , y_val_event = get_target(df_val)\n",
    "\n",
    "_df_train = np.concatenate([x_train , y_train_dur.reshape(-1 , 1) , y_train_event.reshape(-1 , 1)] , axis = 1)\n",
    "_df_test = np.concatenate([x_test , y_test_dur.reshape(-1 , 1) , y_test_event.reshape(-1 , 1)] , axis = 1)\n",
    "_df_val = np.concatenate([x_val , y_val_dur.reshape(-1 , 1) , y_val_event.reshape(-1 , 1)] , axis = 1)\n",
    "\n",
    "_df_train = pd.DataFrame(_df_train , columns = df_train.columns)\n",
    "_df_test = pd.DataFrame(_df_test , columns = df_test.columns)\n",
    "_df_val = pd.DataFrame(_df_val , columns = df_val.columns)\n",
    "\n",
    "_df_train.rename(columns = {'duration':'time_to_event' , 'event':'death'} , inplace = True)\n",
    "_df_test.rename(columns = {'duration':'time_to_event' , 'event':'death'} , inplace = True)\n",
    "_df_val.rename(columns = {'duration':'time_to_event' , 'event':'death'} , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training cluster 0\n",
      "Epoch 50: Training Loss: 0.0037825, Val Loss: 0.0154994\n",
      "Epoch 100: Training Loss: 0.0062350, Val Loss: 0.0096503\n",
      "Epoch 150: Training Loss: 0.0158692, Val Loss: 0.0138773\n",
      "Epoch 200: Training Loss: 0.0118470, Val Loss: 0.0066545\n",
      "Epoch 250: Training Loss: 0.0166348, Val Loss: 0.0098582\n",
      "Epoch 300: Training Loss: 0.0111556, Val Loss: 0.0113556\n",
      "Epoch 350: Training Loss: 0.0069933, Val Loss: 0.0111245\n",
      "Epoch 400: Training Loss: -0.0001434, Val Loss: 0.0129846\n",
      "shapes : (1775, 1775, 1775, 1775)\n",
      "0.562298916354898 0.21533423290168918\n",
      "CPU times: total: 4min 29s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# instantiate - Time Invariant Survival\n",
    "tis = Time_Invariant_Survival(\n",
    "        configs = configs, \n",
    "        train_data = _df_train,\n",
    "        test_data = _df_test, \n",
    "        val_data = _df_val\n",
    ")\n",
    "\n",
    "# fit\n",
    "tis.fit(verbose = True)\n",
    "mean_ , up_ , low_ , y_test_dur , y_test_event = tis.predict() # Visualize -> tis.visualize(mean_ , up_ , low_ , _from = 40 , _to = 50 )\n",
    "tis_cindex , tis_ibs = tis.evaluation(mean_ , y_test_dur , y_test_event, plot = False)\n",
    "print(tis_cindex , tis_ibs)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Fitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[0s / 0s],\t\ttrain_loss: 2.4396,\tval_loss: 2.1060\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 2.0634,\tval_loss: 1.7398\n",
      "2:\t[0s / 0s],\t\ttrain_loss: 1.6757,\tval_loss: 1.4191\n",
      "3:\t[0s / 0s],\t\ttrain_loss: 1.3892,\tval_loss: 1.2767\n",
      "4:\t[0s / 0s],\t\ttrain_loss: 1.2741,\tval_loss: 1.2329\n",
      "5:\t[0s / 0s],\t\ttrain_loss: 1.2275,\tval_loss: 1.2189\n",
      "6:\t[0s / 0s],\t\ttrain_loss: 1.2112,\tval_loss: 1.2136\n",
      "7:\t[0s / 0s],\t\ttrain_loss: 1.1984,\tval_loss: 1.2083\n",
      "8:\t[0s / 1s],\t\ttrain_loss: 1.1863,\tval_loss: 1.2029\n",
      "9:\t[0s / 1s],\t\ttrain_loss: 1.1856,\tval_loss: 1.2010\n",
      "10:\t[0s / 1s],\t\ttrain_loss: 1.1770,\tval_loss: 1.1975\n",
      "11:\t[0s / 1s],\t\ttrain_loss: 1.1664,\tval_loss: 1.1925\n",
      "12:\t[0s / 1s],\t\ttrain_loss: 1.1689,\tval_loss: 1.1903\n",
      "13:\t[0s / 1s],\t\ttrain_loss: 1.1623,\tval_loss: 1.1907\n",
      "14:\t[0s / 1s],\t\ttrain_loss: 1.1579,\tval_loss: 1.1909\n",
      "15:\t[0s / 1s],\t\ttrain_loss: 1.1515,\tval_loss: 1.1908\n",
      "16:\t[0s / 2s],\t\ttrain_loss: 1.1457,\tval_loss: 1.1896\n",
      "17:\t[0s / 2s],\t\ttrain_loss: 1.1472,\tval_loss: 1.1932\n",
      "18:\t[0s / 2s],\t\ttrain_loss: 1.1521,\tval_loss: 1.1892\n",
      "19:\t[0s / 2s],\t\ttrain_loss: 1.1472,\tval_loss: 1.1880\n",
      "20:\t[0s / 2s],\t\ttrain_loss: 1.1512,\tval_loss: 1.1890\n",
      "21:\t[0s / 2s],\t\ttrain_loss: 1.1379,\tval_loss: 1.1879\n",
      "22:\t[0s / 2s],\t\ttrain_loss: 1.1524,\tval_loss: 1.1894\n",
      "23:\t[0s / 2s],\t\ttrain_loss: 1.1395,\tval_loss: 1.1880\n",
      "24:\t[0s / 2s],\t\ttrain_loss: 1.1333,\tval_loss: 1.1911\n",
      "25:\t[0s / 3s],\t\ttrain_loss: 1.1264,\tval_loss: 1.1888\n",
      "26:\t[0s / 3s],\t\ttrain_loss: 1.1398,\tval_loss: 1.1932\n",
      "27:\t[0s / 3s],\t\ttrain_loss: 1.1288,\tval_loss: 1.1950\n",
      "28:\t[0s / 3s],\t\ttrain_loss: 1.1352,\tval_loss: 1.1913\n",
      "29:\t[0s / 3s],\t\ttrain_loss: 1.1239,\tval_loss: 1.1946\n",
      "30:\t[0s / 3s],\t\ttrain_loss: 1.1270,\tval_loss: 1.1950\n",
      "31:\t[0s / 3s],\t\ttrain_loss: 1.1199,\tval_loss: 1.1923\n",
      "shapes : (1775, 1775, 1775, 1775)\n",
      "PyCox: cindex 0.48959402243741756 , ibs 0.20551684267826612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1234/10000 [00:01<00:11, 733.30it/s]\n",
      " 50%|█████     | 50/100 [00:06<00:06,  8.10it/s]\n",
      " 12%|█▏        | 1234/10000 [00:01<00:12, 720.09it/s]\n",
      " 19%|█▉        | 19/100 [00:03<00:13,  6.12it/s]\n",
      " 12%|█▏        | 1234/10000 [00:01<00:12, 712.92it/s]\n",
      " 20%|██        | 20/100 [00:02<00:11,  6.98it/s]\n",
      " 12%|█▏        | 1234/10000 [00:01<00:13, 655.21it/s]\n",
      " 19%|█▉        | 19/100 [00:03<00:13,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes : (1775, 1775, 1775, 1775)\n",
      "Deep Survival Machines: cindex 0.589253120859095 , ibs 0.2180058427814511\n",
      "shapes : (1775, 1775, 1775, 1775)\n",
      "Cox Proportional Hazards: cindex 0.5523647531912814 , ibs 0.19693319946615204\n",
      "shapes : (1775, 1775, 1775, 1775)\n",
      "Weibull Accelerated Failure Time: cindex 0.5504770035153679 , ibs 0.19698153867971116\n",
      "shapes : (1775, 1775, 1775, 1775)\n",
      "Random Survival Forest: cindex 0.6340819658466058 , ibs 0.18112291703494607\n",
      "CPU times: total: 5min 11s\n",
      "Wall time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# instantiate - PyCox\n",
    "pyc = PYC(configs = configs, train_data = _df_train, test_data = _df_test, val_data = _df_val, num_durations = 10)\n",
    "\n",
    "# fit\n",
    "pyc.fit()\n",
    "\n",
    "# eval\n",
    "pyc_cindex , pyc_ibs = pyc.eval()\n",
    "        \n",
    "print(f'PyCox: cindex {pyc_cindex} , ibs {pyc_ibs}')\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# instantiate - Deep Survival Machines\n",
    "dsm = DSM(configs = configs, train_data = _df_train, test_data = _df_test, val_data = _df_val, num_durations = 10)\n",
    "\n",
    "# fit\n",
    "dsm.fit()\n",
    "\n",
    "# eval\n",
    "dsm_cindex , dsm_ibs = dsm.eval()\n",
    "       \n",
    "print(f'Deep Survival Machines: cindex {dsm_cindex} , ibs {dsm_ibs}')\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# instantiate - CPH\n",
    "cph = CPH(configs = configs, train_data = _df_train, test_data = _df_test, val_data = _df_val)\n",
    "\n",
    "# fit\n",
    "cph.fit()\n",
    "# eval\n",
    "cph_cindex , cph_ibs = cph.eval(fitter_is_rsf = False)\n",
    "        \n",
    "print(f'Cox Proportional Hazards: cindex {cph_cindex} , ibs {cph_ibs}')\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# instantiate - AFT\n",
    "aft = AFT(configs = configs, train_data = _df_train, test_data = _df_test, val_data = _df_val)\n",
    "\n",
    "# fit\n",
    "aft.fit()\n",
    "# eval\n",
    "aft_cindex , aft_ibs = aft.eval(fitter_is_rsf = False)\n",
    "        \n",
    "print(f'Weibull Accelerated Failure Time: cindex {aft_cindex} , ibs {aft_ibs}')\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# instantiate - RSF\n",
    "rsf = RSF(configs = configs, train_data = _df_train, test_data = _df_test, val_data = _df_val)\n",
    "\n",
    "# fit\n",
    "rsf.fit()\n",
    "# eval\n",
    "rsf_cindex , rsf_ibs = rsf.eval(fitter_is_rsf = True)\n",
    "\n",
    "\n",
    "print(f'Random Survival Forest: cindex {rsf_cindex} , ibs {rsf_ibs}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
