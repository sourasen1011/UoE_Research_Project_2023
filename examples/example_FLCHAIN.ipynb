{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../nn_survival_analysis')\n",
    "from nn_survival_analysis.general_utils import *\n",
    "from nn_survival_analysis.model_utils import *\n",
    "from nn_survival_analysis.losses import *\n",
    "from nn_survival_analysis.models import *\n",
    "from nn_survival_analysis.other_nn_models import *\n",
    "from nn_survival_analysis.time_invariant_surv import *\n",
    "from nn_survival_analysis.time_variant_surv import *\n",
    "from nn_survival_analysis.traditional_models import *\n",
    "import scipy\n",
    "\n",
    "# define sigmoid function - will be handy later\n",
    "sigmoid = lambda z : 1 / (1 + np.exp(-z))\n",
    "\n",
    "config_file_path = '../nn_survival_analysis/config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flchain = pd.read_csv(\"../resources/other_data/FLCHAIN.csv\")\n",
    "flchain.rename(columns = {'futime':'time_to_event'} , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_splitter(df , test_size = 0.2 , val_size = 0.25 , duration_col = 'futime' , event_col = 'death'):\n",
    "    df_test = df.sample(frac=test_size)\n",
    "    df_train = df.drop(df_test.index)\n",
    "\n",
    "    df_val = df_train.sample(frac=val_size)\n",
    "    df_train = df_train.drop(df_val.index)\n",
    "\n",
    "    return df_train , df_val , df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_sex_data.shape (7874, 1)\n",
      "encoded_chap_data.shape (7874, 1)\n",
      "flchain_mod.shape (7874, 11)\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the OneHotEncoder class\n",
    "encoder1 = LabelEncoder()\n",
    "encoder2 = LabelEncoder()\n",
    "\n",
    "# Fit and transform the data\n",
    "encoded_sex_data = encoder1.fit_transform(flchain['sex'])\n",
    "encoded_sex_data = pd.DataFrame(encoded_sex_data.reshape(-1 , 1) , columns = ['sex']) \n",
    "print(f'encoded_sex_data.shape {encoded_sex_data.shape}')\n",
    "\n",
    "# Fit and transform the data\n",
    "encoded_chap_data = encoder2.fit_transform(flchain['chapter'])\n",
    "encoded_chap_data = pd.DataFrame(encoded_chap_data.reshape(-1 , 1) , columns = ['chapter'])\n",
    "print(f'encoded_chap_data.shape {encoded_chap_data.shape}')\n",
    "\n",
    "flchain_mod = pd.DataFrame(\n",
    "    pd.concat(\n",
    "        [\n",
    "            encoded_sex_data , \n",
    "            encoded_chap_data , \n",
    "            flchain[['age' , 'sample.yr' , 'kappa' , 'lambda' , 'flc.grp' , 'creatinine' , 'mgus']] , \n",
    "            flchain[['time_to_event' , 'death']]\n",
    "        ] ,\n",
    "        axis = 1\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(f'flchain_mod.shape {flchain_mod.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(df):\n",
    "    _imputer = SimpleImputer(strategy='mean')\n",
    "    df_imp = pd.DataFrame(_imputer.fit_transform(df) , columns = df.columns)\n",
    "    return df_imp\n",
    "\n",
    "def scale(df):\n",
    "    _scaler = StandardScaler()\n",
    "    scale_cols = ['age' , 'sample.yr' , 'kappa' , 'lambda' , 'flc.grp' , 'creatinine' , 'mgus']\n",
    "    unscaled_cols = [col for col in df.columns if col not in scale_cols]\n",
    "    scaled = pd.DataFrame(_scaler.fit_transform(df[scale_cols]) , columns = scale_cols)\n",
    "    return pd.concat([scaled , df[unscaled_cols]] , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train , df_test , df_val = train_test_splitter(flchain_mod)\n",
    "preprocess = lambda df: scale(impute(df))\n",
    "# preprocess train\n",
    "x_train = preprocess(df_train)\n",
    "\n",
    "# preprocess test\n",
    "x_test = preprocess(df_test)\n",
    "\n",
    "# preprocess val\n",
    "x_val = preprocess(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training cluster 0\n",
      "Epoch 50: Training Loss: -0.3624409, Val Loss: -0.3709950\n",
      "Epoch 100: Training Loss: -0.4037809, Val Loss: -0.3759740\n",
      "Epoch 150: Training Loss: -0.3937853, Val Loss: -0.3873606\n",
      "Epoch 200: Training Loss: -0.4089943, Val Loss: -0.3910943\n",
      "Epoch 250: Training Loss: -0.3807949, Val Loss: -0.3982277\n",
      "Epoch 300: Training Loss: -0.3711786, Val Loss: -0.3942183\n",
      "Epoch 350: Training Loss: -0.3999810, Val Loss: -0.4037854\n",
      "Epoch 400: Training Loss: -0.4269630, Val Loss: -0.4099655\n",
      "Epoch 450: Training Loss: -0.4170119, Val Loss: -0.4138110\n",
      "Epoch 500: Training Loss: -0.3885909, Val Loss: -0.4136161\n",
      "Epoch 550: Training Loss: -0.4063324, Val Loss: -0.4166683\n",
      "Epoch 600: Training Loss: -0.4347412, Val Loss: -0.4206513\n",
      "shapes : (1575, 1575, 1575, 1575)\n",
      "0.9256305351717776 0.11718056272459135\n",
      "CPU times: total: 8min 45s\n",
      "Wall time: 1min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get configs\n",
    "with open(config_file_path, \"r\") as file:\n",
    "        configs = json.load(file)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# instantiate - Time Invariant Survival\n",
    "tis = Time_Invariant_Survival(\n",
    "        configs = configs, \n",
    "        train_data = x_train,\n",
    "        test_data = x_test, \n",
    "        val_data = x_val\n",
    ")\n",
    "\n",
    "# fit\n",
    "tis.fit(verbose = True)\n",
    "mean_ , up_ , low_ , y_test_dur , y_test_event = tis.predict() # Visualize -> tis.visualize(mean_ , up_ , low_ , _from = 40 , _to = 50 )\n",
    "tis_cindex , tis_ibs = tis.evaluation(mean_ , y_test_dur , y_test_event, plot = False)\n",
    "print(tis_cindex , tis_ibs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Fitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[0s / 0s],\t\ttrain_loss: 4.3831,\tval_loss: 3.2037\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 3.5251,\tval_loss: 2.6299\n",
      "2:\t[0s / 0s],\t\ttrain_loss: 2.6921,\tval_loss: 1.9053\n",
      "3:\t[0s / 0s],\t\ttrain_loss: 1.7268,\tval_loss: 1.2249\n",
      "4:\t[0s / 0s],\t\ttrain_loss: 1.2080,\tval_loss: 0.8478\n",
      "5:\t[0s / 0s],\t\ttrain_loss: 0.9761,\tval_loss: 0.7567\n",
      "6:\t[0s / 0s],\t\ttrain_loss: 0.8800,\tval_loss: 0.7093\n",
      "7:\t[0s / 0s],\t\ttrain_loss: 0.8411,\tval_loss: 0.6674\n",
      "8:\t[0s / 0s],\t\ttrain_loss: 0.8069,\tval_loss: 0.6898\n",
      "9:\t[0s / 0s],\t\ttrain_loss: 0.7869,\tval_loss: 0.6856\n",
      "10:\t[0s / 0s],\t\ttrain_loss: 0.7547,\tval_loss: 0.6592\n",
      "11:\t[0s / 1s],\t\ttrain_loss: 0.7491,\tval_loss: 0.6258\n",
      "12:\t[0s / 1s],\t\ttrain_loss: 0.7635,\tval_loss: 0.6127\n",
      "13:\t[0s / 1s],\t\ttrain_loss: 0.7453,\tval_loss: 0.6143\n",
      "14:\t[0s / 1s],\t\ttrain_loss: 0.7055,\tval_loss: 0.6260\n",
      "15:\t[0s / 1s],\t\ttrain_loss: 0.7014,\tval_loss: 0.6018\n",
      "16:\t[0s / 1s],\t\ttrain_loss: 0.7028,\tval_loss: 0.6101\n",
      "17:\t[0s / 1s],\t\ttrain_loss: 0.7031,\tval_loss: 0.5999\n",
      "18:\t[0s / 1s],\t\ttrain_loss: 0.7040,\tval_loss: 0.6110\n",
      "19:\t[0s / 1s],\t\ttrain_loss: 0.7156,\tval_loss: 0.6055\n",
      "20:\t[0s / 1s],\t\ttrain_loss: 0.6853,\tval_loss: 0.6020\n",
      "21:\t[0s / 1s],\t\ttrain_loss: 0.6832,\tval_loss: 0.6079\n",
      "22:\t[0s / 1s],\t\ttrain_loss: 0.6734,\tval_loss: 0.6074\n",
      "23:\t[0s / 1s],\t\ttrain_loss: 0.6547,\tval_loss: 0.5915\n",
      "24:\t[0s / 1s],\t\ttrain_loss: 0.6538,\tval_loss: 0.6043\n",
      "25:\t[0s / 2s],\t\ttrain_loss: 0.6462,\tval_loss: 0.5974\n",
      "26:\t[0s / 2s],\t\ttrain_loss: 0.6518,\tval_loss: 0.5883\n",
      "27:\t[0s / 2s],\t\ttrain_loss: 0.6452,\tval_loss: 0.5962\n",
      "28:\t[0s / 2s],\t\ttrain_loss: 0.6348,\tval_loss: 0.5786\n",
      "29:\t[0s / 2s],\t\ttrain_loss: 0.6394,\tval_loss: 0.6050\n",
      "30:\t[0s / 2s],\t\ttrain_loss: 0.6408,\tval_loss: 0.5864\n",
      "31:\t[0s / 2s],\t\ttrain_loss: 0.6210,\tval_loss: 0.5950\n",
      "32:\t[0s / 2s],\t\ttrain_loss: 0.6213,\tval_loss: 0.5971\n",
      "33:\t[0s / 2s],\t\ttrain_loss: 0.6401,\tval_loss: 0.5935\n",
      "34:\t[0s / 2s],\t\ttrain_loss: 0.6102,\tval_loss: 0.5868\n",
      "35:\t[0s / 2s],\t\ttrain_loss: 0.6061,\tval_loss: 0.5885\n",
      "36:\t[0s / 2s],\t\ttrain_loss: 0.6127,\tval_loss: 0.6002\n",
      "37:\t[0s / 2s],\t\ttrain_loss: 0.6171,\tval_loss: 0.5924\n",
      "38:\t[0s / 3s],\t\ttrain_loss: 0.6089,\tval_loss: 0.5853\n",
      "shapes : (1575, 1575, 1575, 1575)\n",
      "PyCox: cindex 0.8796136360746791 , ibs 0.050691622963506405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:13<00:00, 724.38it/s]\n",
      "100%|██████████| 100/100 [00:09<00:00, 10.01it/s]\n",
      "100%|██████████| 10000/10000 [00:13<00:00, 761.94it/s]\n",
      "100%|██████████| 100/100 [00:11<00:00,  9.07it/s]\n",
      "100%|██████████| 10000/10000 [00:13<00:00, 761.10it/s]\n",
      "100%|██████████| 100/100 [00:10<00:00,  9.12it/s]\n",
      "100%|██████████| 10000/10000 [00:13<00:00, 740.42it/s]\n",
      "100%|██████████| 100/100 [00:12<00:00,  8.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes : (1575, 1575, 1575, 1575)\n",
      "Deep Survival Machines: cindex 0.0 , ibs nan\n",
      "shapes : (1575, 1575, 1575, 1575)\n",
      "Cox Proportional Hazards: cindex 0.8955044690129665 , ibs 0.08126015193916596\n",
      "shapes : (1575, 1575, 1575, 1575)\n",
      "Weibull Accelerated Failure Time: cindex 0.894211261440437 , ibs 0.08147364695364535\n",
      "shapes : (1575, 1575, 1575, 1575)\n",
      "Random Survival Forest: cindex 0.9308624350444722 , ibs 0.07124332232588824\n",
      "CPU times: total: 11min 42s\n",
      "Wall time: 3min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# instantiate - PyCox\n",
    "pyc = PYC(configs = configs, train_data = x_train, test_data = x_test, val_data = x_val, num_durations = 10)\n",
    "\n",
    "# fit\n",
    "pyc.fit()\n",
    "\n",
    "# eval\n",
    "pyc_cindex , pyc_ibs = pyc.eval()\n",
    "        \n",
    "print(f'PyCox: cindex {pyc_cindex} , ibs {pyc_ibs}')\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# instantiate - Deep Survival Machines\n",
    "dsm = DSM(configs = configs, train_data = x_train, test_data = x_test, val_data = x_val, num_durations = 10)\n",
    "\n",
    "# fit\n",
    "dsm.fit()\n",
    "\n",
    "# eval\n",
    "dsm_cindex , dsm_ibs = dsm.eval()\n",
    "       \n",
    "print(f'Deep Survival Machines: cindex {dsm_cindex} , ibs {dsm_ibs}')\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# instantiate - CPH\n",
    "cph = CPH(configs = configs, train_data = x_train, test_data = x_test, val_data = x_val)\n",
    "\n",
    "# fit\n",
    "cph.fit()\n",
    "# eval\n",
    "cph_cindex , cph_ibs = cph.eval(fitter_is_rsf = False)\n",
    "        \n",
    "print(f'Cox Proportional Hazards: cindex {cph_cindex} , ibs {cph_ibs}')\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# instantiate - AFT\n",
    "aft = AFT(configs = configs, train_data = x_train, test_data = x_test, val_data = x_val)\n",
    "\n",
    "# fit\n",
    "aft.fit()\n",
    "# eval\n",
    "aft_cindex , aft_ibs = aft.eval(fitter_is_rsf = False)\n",
    "        \n",
    "print(f'Weibull Accelerated Failure Time: cindex {aft_cindex} , ibs {aft_ibs}')\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# instantiate - RSF\n",
    "rsf = RSF(configs = configs, train_data = x_train, test_data = x_test, val_data = x_val)\n",
    "\n",
    "# fit\n",
    "rsf.fit()\n",
    "# eval\n",
    "rsf_cindex , rsf_ibs = rsf.eval(fitter_is_rsf = True)\n",
    "\n",
    "\n",
    "print(f'Random Survival Forest: cindex {rsf_cindex} , ibs {rsf_ibs}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
