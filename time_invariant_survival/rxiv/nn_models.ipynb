{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "from general_utils import *\n",
    "\n",
    "# NN\n",
    "from pycox.datasets import metabric\n",
    "from pycox.models import LogisticHazard\n",
    "import torchtuples as tt\n",
    "from auton_survival.models.dsm import DeepSurvivalMachines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get configs\n",
    "with open(config_file_path, \"r\") as file:\n",
    "    configs = json.load(file)\n",
    "\n",
    "# Read the pickled DataFrames\n",
    "with open('../05_preprocessing_emr_data/data/x_train.pickle', 'rb') as file:\n",
    "    x_train = pickle.load(file)\n",
    "with open('../05_preprocessing_emr_data/data/x_test.pickle', 'rb') as file:\n",
    "    x_test = pickle.load(file)\n",
    "with open('../05_preprocessing_emr_data/data/x_val.pickle', 'rb') as file:\n",
    "    x_val = pickle.load(file)\n",
    "\n",
    "# Read the pickled DataFrame\n",
    "with open('../05_preprocessing_emr_data/data/consolidated_pat_tbl.pickle', 'rb') as file:\n",
    "    consolidated_pat_tbl = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_fitter:\n",
    "    '''\n",
    "    simple wrapper class for NN models\n",
    "    '''\n",
    "    def __init__(self , configs , train_data , test_data , val_data , num_durations):\n",
    "        self.configs = configs\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.val_data = val_data\n",
    "        # some aux vars\n",
    "        self.num_durations = num_durations\n",
    "        self.labtrans = LogisticHazard.label_transform(self.num_durations)\n",
    "\n",
    "        # targets\n",
    "        self.y_train = self.labtrans.fit_transform(*get_target(self.train_data))\n",
    "        self.y_val = self.labtrans.transform(*get_target(self.val_data))\n",
    "        self.out_features = self.labtrans.out_features\n",
    "\n",
    "        # state var\n",
    "        self.fitted = False\n",
    "        self.fitter = None\n",
    "    \n",
    "    \n",
    "    def eval(self):\n",
    "        if not self.fitted:\n",
    "            raise Exception('Model not fitted yet!')\n",
    "\n",
    "        # _surv = self.fitter.predict_surv_df(self.test_data.iloc[: , :-2].to_numpy().astype('float32'))\n",
    "        ev = EvalSurv(pd.DataFrame(self._surv), self.test_data['time_to_event'].to_numpy(), self.test_data['death'].to_numpy(), censor_surv='km')\n",
    "\n",
    "        # get time grid\n",
    "        time_grid_div = self.configs['time_invariant']['eval']['time_grid_div']\n",
    "        time_grid = np.linspace(self.test_data['time_to_event'].min(), self.test_data['time_to_event'].max(), time_grid_div)\n",
    "        # get metrics\n",
    "        cindex = ev.concordance_td('antolini')\n",
    "        ibs = ev.integrated_brier_score(time_grid)\n",
    "        return cindex , ibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pycox_fitter(nn_fitter):\n",
    "    '''\n",
    "    simple class for pycox logisitc hazards model\n",
    "    '''\n",
    "    def __init__(self , configs , train_data , test_data , val_data , num_durations):\n",
    "        super(pycox_fitter , self).__init__(configs , train_data , test_data , val_data , num_durations)\n",
    "\n",
    "    def fit(self):\n",
    "        in_features = self.train_data.iloc[: , :-2].shape[1]\n",
    "        num_nodes = [256,256]\n",
    "\n",
    "        batch_norm = True\n",
    "        dropout = 0.5\n",
    "\n",
    "        train = (self.train_data.iloc[: , :-2].to_numpy().astype('float32'), self.y_train)\n",
    "        val = (self.val_data.iloc[: , :-2].to_numpy().astype('float32'), self.y_val)\n",
    "\n",
    "        net = tt.practical.MLPVanilla(in_features, num_nodes, self.out_features, batch_norm, dropout)\n",
    "\n",
    "        model = LogisticHazard(net, tt.optim.Adam(0.002), duration_index=self.labtrans.cuts)\n",
    "\n",
    "        batch_size = 256\n",
    "        epochs = 500\n",
    "        callbacks = [tt.cb.EarlyStopping()]\n",
    "\n",
    "        log = model.fit(self.train_data.iloc[:,:-2].to_numpy().astype('float32'), self.y_train, batch_size, epochs, callbacks, val_data=val)\n",
    "        \n",
    "        # assign the fitted model to a class attr\n",
    "        self.fitter = model\n",
    "        # change state var\n",
    "        self.fitted = True\n",
    "        \n",
    "        # predict\n",
    "        self._surv = self.fitter.predict_surv_df(self.test_data.iloc[: , :-2].to_numpy().astype('float32'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[0s / 0s],\t\ttrain_loss: 1.7261,\tval_loss: 1.4946\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 1.5203,\tval_loss: 1.3216\n",
      "2:\t[0s / 0s],\t\ttrain_loss: 1.3096,\tval_loss: 1.1028\n",
      "3:\t[0s / 0s],\t\ttrain_loss: 1.0920,\tval_loss: 0.8955\n",
      "4:\t[0s / 0s],\t\ttrain_loss: 0.9154,\tval_loss: 0.7668\n",
      "5:\t[0s / 0s],\t\ttrain_loss: 0.8226,\tval_loss: 0.7208\n",
      "6:\t[0s / 0s],\t\ttrain_loss: 0.7808,\tval_loss: 0.7026\n",
      "7:\t[0s / 0s],\t\ttrain_loss: 0.7549,\tval_loss: 0.6867\n",
      "8:\t[0s / 0s],\t\ttrain_loss: 0.7291,\tval_loss: 0.6825\n",
      "9:\t[0s / 0s],\t\ttrain_loss: 0.7443,\tval_loss: 0.6784\n",
      "10:\t[0s / 0s],\t\ttrain_loss: 0.7350,\tval_loss: 0.6818\n",
      "11:\t[0s / 0s],\t\ttrain_loss: 0.7154,\tval_loss: 0.6762\n",
      "12:\t[0s / 0s],\t\ttrain_loss: 0.7077,\tval_loss: 0.6751\n",
      "13:\t[0s / 1s],\t\ttrain_loss: 0.7082,\tval_loss: 0.6699\n",
      "14:\t[0s / 1s],\t\ttrain_loss: 0.6932,\tval_loss: 0.6702\n",
      "15:\t[0s / 1s],\t\ttrain_loss: 0.6894,\tval_loss: 0.6670\n",
      "16:\t[0s / 1s],\t\ttrain_loss: 0.6955,\tval_loss: 0.6731\n",
      "17:\t[0s / 1s],\t\ttrain_loss: 0.6831,\tval_loss: 0.6673\n",
      "18:\t[0s / 1s],\t\ttrain_loss: 0.6907,\tval_loss: 0.6661\n",
      "19:\t[0s / 1s],\t\ttrain_loss: 0.6860,\tval_loss: 0.6681\n",
      "20:\t[0s / 1s],\t\ttrain_loss: 0.6755,\tval_loss: 0.6614\n",
      "21:\t[0s / 1s],\t\ttrain_loss: 0.6807,\tval_loss: 0.6649\n",
      "22:\t[0s / 1s],\t\ttrain_loss: 0.6701,\tval_loss: 0.6647\n",
      "23:\t[0s / 1s],\t\ttrain_loss: 0.6588,\tval_loss: 0.6633\n",
      "24:\t[0s / 1s],\t\ttrain_loss: 0.6674,\tval_loss: 0.6616\n",
      "25:\t[0s / 1s],\t\ttrain_loss: 0.6635,\tval_loss: 0.6618\n",
      "26:\t[0s / 1s],\t\ttrain_loss: 0.6626,\tval_loss: 0.6616\n",
      "27:\t[0s / 1s],\t\ttrain_loss: 0.6706,\tval_loss: 0.6656\n",
      "28:\t[0s / 2s],\t\ttrain_loss: 0.6522,\tval_loss: 0.6636\n",
      "29:\t[0s / 2s],\t\ttrain_loss: 0.6583,\tval_loss: 0.6617\n",
      "30:\t[0s / 2s],\t\ttrain_loss: 0.6603,\tval_loss: 0.6583\n",
      "31:\t[0s / 2s],\t\ttrain_loss: 0.6478,\tval_loss: 0.6616\n",
      "32:\t[0s / 2s],\t\ttrain_loss: 0.6516,\tval_loss: 0.6631\n",
      "33:\t[0s / 2s],\t\ttrain_loss: 0.6455,\tval_loss: 0.6637\n",
      "34:\t[0s / 2s],\t\ttrain_loss: 0.6404,\tval_loss: 0.6634\n",
      "35:\t[0s / 2s],\t\ttrain_loss: 0.6450,\tval_loss: 0.6622\n",
      "36:\t[0s / 2s],\t\ttrain_loss: 0.6381,\tval_loss: 0.6629\n",
      "37:\t[0s / 2s],\t\ttrain_loss: 0.6436,\tval_loss: 0.6639\n",
      "38:\t[0s / 2s],\t\ttrain_loss: 0.6402,\tval_loss: 0.6637\n",
      "39:\t[0s / 2s],\t\ttrain_loss: 0.6444,\tval_loss: 0.6657\n",
      "40:\t[0s / 2s],\t\ttrain_loss: 0.6304,\tval_loss: 0.6668\n",
      "shapes : (1060, 1060, 1060, 1060)\n"
     ]
    }
   ],
   "source": [
    "# instantiate - CPH\n",
    "pyc = pycox_fitter(configs = configs, train_data = x_train, test_data = x_test, val_data = x_val, num_durations = 10)\n",
    "\n",
    "# fit\n",
    "pyc.fit()\n",
    "\n",
    "# eval\n",
    "pyc_cindex , pyc_ibs = pyc.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dsm_fitter(nn_fitter):\n",
    "    '''\n",
    "    simple class for DSM model\n",
    "    '''\n",
    "    def __init__(self , configs , train_data , test_data , val_data , num_durations):\n",
    "        super(dsm_fitter , self).__init__(configs , train_data , test_data , val_data , num_durations)\n",
    "\n",
    "    def fit(self):\n",
    "        times = list(self.labtrans.cuts)\n",
    "\n",
    "        param_grid = {'k' : [3,4],\n",
    "              'distribution' : ['LogNormal'],\n",
    "              'learning_rate' : [1e-3],\n",
    "              'layers' : [[100],[100,100]]\n",
    "             }\n",
    "\n",
    "        params = ParameterGrid(param_grid)\n",
    "\n",
    "        models = []\n",
    "        for param in params:\n",
    "            model = DeepSurvivalMachines(k = param['k'],\n",
    "                                        distribution = param['distribution'],\n",
    "                                        layers = param['layers'])\n",
    "            # The fit method is called to train the model\n",
    "            model.fit(x_train.iloc[: , :-2].to_numpy(), x_train['time_to_event'].to_numpy(), x_train['death'].to_numpy() ,\n",
    "                    iters = 100 , \n",
    "                    learning_rate = param['learning_rate']\n",
    "                    )\n",
    "            models.append(\n",
    "                [\n",
    "                    [\n",
    "                        model.compute_nll(x_val.iloc[: , :-2].to_numpy(), x_val['time_to_event'].to_numpy(), x_val['death'].to_numpy()), \n",
    "                        model,\n",
    "                        param\n",
    "                    ]\n",
    "                ]\n",
    "            )\n",
    "        best_model = min(models)\n",
    "        model = best_model[0][1]\n",
    "        param = best_model[0][2]\n",
    "        self.best_param = param\n",
    "        \n",
    "        # assign the fitted model to a class attr\n",
    "        self.fitter = model\n",
    "        # change state var\n",
    "        self.fitted = True\n",
    "        \n",
    "        # predict\n",
    "        out_survival = model.predict_survival(self.test_data.iloc[: , :-2].to_numpy().astype('float64'), times)\n",
    "        self._surv = out_survival.T    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1444/10000 [00:01<00:11, 775.82it/s]\n",
      "  8%|▊         | 8/100 [00:00<00:11,  8.30it/s]\n",
      " 14%|█▍        | 1444/10000 [00:01<00:10, 785.69it/s]\n",
      "  7%|▋         | 7/100 [00:00<00:12,  7.20it/s]\n",
      " 14%|█▍        | 1444/10000 [00:01<00:11, 777.46it/s]\n",
      " 14%|█▍        | 14/100 [00:01<00:09,  8.81it/s]\n",
      " 14%|█▍        | 1444/10000 [00:01<00:10, 785.93it/s]\n",
      "  6%|▌         | 6/100 [00:00<00:14,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes : (1060, 1060, 1060, 1060)\n"
     ]
    }
   ],
   "source": [
    "# instantiate - CPH\n",
    "dsm = dsm_fitter(configs = configs, train_data = x_train, test_data = x_test, val_data = x_val, num_durations = 10)\n",
    "\n",
    "# fit\n",
    "dsm.fit()\n",
    "\n",
    "# eval\n",
    "dsm_cindex , dsm_ibs = dsm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.645945337535422, 0.27302147855965664)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsm_cindex , dsm_ibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'distribution': 'LogNormal', 'k': 3, 'layers': [100], 'learning_rate': 0.001}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsm.best_param\n",
    "# {'distribution': 'LogNormal', 'k': 4, 'layers': [100], 'learning_rate': 0.001}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
