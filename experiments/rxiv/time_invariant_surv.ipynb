{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "from general_utils import *\n",
    "from model_utils import *\n",
    "from losses import *\n",
    "from models import *\n",
    "\n",
    "# Get configs\n",
    "with open(config_file_path, \"r\") as file:\n",
    "    configs = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time_Invariant_Survival:\n",
    "    '''\n",
    "    Class for fitting, testing, evaluating and explaining survival distributions\n",
    "    '''\n",
    "    def __init__(self, configs, train_data, test_data, val_data):\n",
    "        '''\n",
    "        configs - dictionary created from loaded json file that contains all configs parsed from config file\n",
    "        train_data - self.explanatory\n",
    "        test_data - self.explanatory\n",
    "        val_data - self.explanatory\n",
    "        '''\n",
    "        self.configs = configs\n",
    "        # assumes that last two columns contain survival time and survival status\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.val_data = val_data\n",
    "\n",
    "        # state vars\n",
    "        self.clustered = False\n",
    "        self.fitted = False\n",
    "        self.discretized = False\n",
    "        self.predicted = False\n",
    "        \n",
    "        # read from config file\n",
    "        self._clusters = self.configs['time_invariant']['training']['clusters'] # Specify the number of clusters (K)\n",
    "        self.q_cuts = self.configs['time_invariant']['training']['q_cuts']    # number of discretized durations\n",
    "        self.hidden_size = self.configs['time_invariant']['training']['hidden_size'] # hidden size of MLP\n",
    "        self.output_size = self.q_cuts # same as discretizations\n",
    "        self.alpha = self.configs['time_invariant']['training']['alpha'] # trade off between two loss functions\n",
    "        self.batch_size = self.configs['time_invariant']['training']['batch_size'] # batch size for NN\n",
    "        self.num_epochs = self.configs['time_invariant']['training']['num_epochs'] # Number of epochs for NN\n",
    "        self.learning_rate = self.configs['time_invariant']['training']['learning_rate'] # LR for NN\n",
    "        self.shuffle = self.configs['time_invariant']['training']['shuffle'] # shuffle for Dataloader class\n",
    "        self.patience = self.configs['time_invariant']['training']['patience'] # patience for early stopping\n",
    "        self.dropout = self.configs['time_invariant']['training']['dropout'] # dropout for training and MC dropout    \n",
    "        \n",
    "    def cluster_train(self):\n",
    "        '''\n",
    "        cluster the training data\n",
    "        '''\n",
    "        # Create a KMeans instance and fit the data\n",
    "        self.kmeans = KMeans(n_clusters=self._clusters , n_init = 'auto')\n",
    "        self.kmeans.fit(self.train_data.iloc[: , :-2])\n",
    "\n",
    "        # Add cluster\n",
    "        self.train_data['cluster'] = self.kmeans.labels_\n",
    "\n",
    "        # grouping based on cluster\n",
    "        train_grouped = self.train_data.groupby('cluster')\n",
    "\n",
    "        # drop cluster to avoid shape problems - no need to make this an object attr\n",
    "        self.train_data.drop('cluster' , axis = 1 , inplace = True)\n",
    "\n",
    "        # test\n",
    "        assert self.train_data.shape[1] == self.val_data.shape[1] == self.test_data.shape[1]\n",
    "\n",
    "        # change state\n",
    "        self.clustered = True\n",
    "\n",
    "        return train_grouped\n",
    "\n",
    "    def cluster_other(self , _data):\n",
    "        '''\n",
    "        uses the fitted kmeans object\n",
    "        _data: can be test or validation data\n",
    "        '''\n",
    "        if not self.clustered:\n",
    "            raise Exception(\"Clustering is not done yet!\")\n",
    "\n",
    "        _clusters = self.kmeans.predict(_data.iloc[: , :-2])\n",
    "        \n",
    "        # Add cluster\n",
    "        _data['cluster'] = _clusters\n",
    "\n",
    "        _grouped = _data.groupby('cluster')\n",
    "\n",
    "        # drop cluster to avoid shape problems\n",
    "        _data.drop('cluster' , axis = 1 , inplace = True)\n",
    "\n",
    "        return _grouped\n",
    "\n",
    "    def fit(self , verbose = False):\n",
    "        '''\n",
    "        fitter function\n",
    "        verbose: print on or off\n",
    "        '''\n",
    "        input_size = self.train_data.iloc[: , :-2].shape[1]\n",
    "\n",
    "        # build net(s)\n",
    "        self.nets = []\n",
    "        \n",
    "        # init loss\n",
    "        l = generic_Loss()\n",
    "\n",
    "        # init besst loss for early stopping\n",
    "        best_loss = np.Inf\n",
    "\n",
    "        # get clusters\n",
    "        x_train_grouped = self.cluster_train() #train\n",
    "        x_val_grouped = self.cluster_other(self.val_data) #val\n",
    "\n",
    "        # init as many networks as there are clusters\n",
    "        for grp in x_train_grouped.groups.keys():\n",
    "            # init net\n",
    "            mlp = MLP(input_size , self.hidden_size , self.output_size , self.dropout)\n",
    "            \n",
    "            # init optim\n",
    "            optimizer = torch.optim.Adam(mlp.parameters() , lr = self.learning_rate)\n",
    "\n",
    "            # append\n",
    "            self.nets.append((mlp , optimizer))\n",
    "\n",
    "        for i , grp in enumerate(x_train_grouped.groups.keys()): \n",
    "            if verbose: print(f'training cluster {i}')\n",
    "            \n",
    "            # get features, death time and event indicator\n",
    "            features = x_train_grouped.get_group(grp)\n",
    "            \n",
    "            y_train_dur , y_train_event = get_target(features)\n",
    "\n",
    "            t_train = Transforms(durations = y_train_dur)\n",
    "            dur_idx = t_train.discrete_transform(_cuts = self.q_cuts)\n",
    "\n",
    "            features = features.iloc[: , :-2].to_numpy() # curtail  features column\n",
    "\n",
    "            # Create an instance of your custom dataset\n",
    "            dataset = MyDataset(features, dur_idx , y_train_dur , y_train_event) # need to change outcomes[0] to indexed version\n",
    "            dataloader = DataLoader(dataset, batch_size = self.batch_size, shuffle = self.shuffle)\n",
    "            \n",
    "            # get mlp and optimizer\n",
    "            net = self.nets[i][0]\n",
    "            optimizer = self.nets[i][1]\n",
    "\n",
    "            # Training loop\n",
    "            for epoch in range(self.num_epochs):\n",
    "                for batch_id , (patient_image , dur_idx , dur , eve) in enumerate(dataloader):\n",
    "\n",
    "                    # Prime for training\n",
    "                    net.train()\n",
    "                    \n",
    "                    # forward pass\n",
    "                    phi_train = net(patient_image)\n",
    "\n",
    "                    # make survival matrix\n",
    "                    sm = Surv_Matrix(duration_index = dur_idx, events = eve , q_cuts = self.q_cuts)\n",
    "                    surv_mat = sm.make_survival_matrix()        \n",
    "\n",
    "                    # get loss\n",
    "                    loss_1 = l.nll_logistic_hazard(\n",
    "                        logits = phi_train , \n",
    "                        targets = surv_mat , \n",
    "                        dur_idx = dur_idx\n",
    "                        )\n",
    "                    loss_2 = l.c_index_lbo_loss(\n",
    "                        logits = phi_train , \n",
    "                        times = dur , \n",
    "                        events = eve\n",
    "                        )\n",
    "                    \n",
    "                    # combine\n",
    "                    loss = self.alpha*loss_1 + (1-self.alpha)*(loss_2)\n",
    "\n",
    "                    # backward pass\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # Early stopping\n",
    "                    with torch.no_grad():\n",
    "                        # get features, death time and event indicator\n",
    "                        val_features = x_val_grouped.get_group(grp)\n",
    "                        \n",
    "                        y_val_dur , y_val_event = get_target(val_features)\n",
    "\n",
    "                        t_val = Transforms(durations = y_val_dur)\n",
    "                        dur_idx_val = t_val.discrete_transform(_cuts = self.q_cuts)\n",
    "\n",
    "                        val_features = val_features.iloc[: , :-2].to_numpy() # curtail  features column\n",
    "                        \n",
    "                        # prepare validation data with survival matrix (reqd for L1 (NLL Loss))\n",
    "                        sm_val = Surv_Matrix(duration_index = dur_idx_val , events = y_val_event , q_cuts = self.q_cuts)\n",
    "                        surv_mat_val = sm_val.make_survival_matrix()  \n",
    "                            \n",
    "                        # pass through the net\n",
    "                        phi_val = net(val_features)\n",
    "                        val_loss_1 = l.nll_logistic_hazard(\n",
    "                            logits = phi_val, \n",
    "                            targets = surv_mat_val , \n",
    "                            dur_idx = dur_idx_val\n",
    "                            )\n",
    "                        val_loss_2 = l.c_index_lbo_loss(\n",
    "                            logits = phi_val , \n",
    "                            times = torch.Tensor(y_val_dur) , \n",
    "                            events = torch.Tensor(y_val_event)\n",
    "                            )\n",
    "\n",
    "                        # combine\n",
    "                        val_loss = self.alpha*val_loss_1 + (1-self.alpha)*(val_loss_2)\n",
    "\n",
    "                    # Check if validation loss has improved\n",
    "                    if val_loss < best_loss:\n",
    "                        best_loss = val_loss\n",
    "                        counter = 0\n",
    "                    else:\n",
    "                        counter += 1\n",
    "\n",
    "                    # Check if early stopping condition is met\n",
    "                    if counter >= self.patience:\n",
    "                        # print(f\"Early stopping at epoch {epoch}.\")\n",
    "                        break\n",
    "                \n",
    "                # control verbosity\n",
    "                if verbose:\n",
    "                    if (epoch%50==0): \n",
    "                        print(f\"Epoch {epoch}: Training Loss: {loss.item():.7f}, Val Loss: {val_loss.item():.7f}\") \n",
    "        \n",
    "        # change state\n",
    "        self.fitted = True\n",
    "\n",
    "    def predict(self):\n",
    "        '''\n",
    "        this is the prediction suite\n",
    "        '''       \n",
    "        if not self.fitted:\n",
    "            raise Exception(\"Model isn't fitted yet!\")\n",
    "\n",
    "        x_test_grouped = self.cluster_other(self.test_data)\n",
    "\n",
    "        # Testing\n",
    "        surv = [] # length will be equal to number of cluster\n",
    "        mc_iter = self.configs['time_invariant']['testing']['mc_iter']\n",
    "        conf_int_lower = self.configs['time_invariant']['testing']['conf_int_lower']\n",
    "        conf_int_upper = self.configs['time_invariant']['testing']['conf_int_upper']\n",
    "        \n",
    "\n",
    "        # init empty lists for duration and event \n",
    "        y_test_dur = []\n",
    "        y_test_event = []\n",
    "\n",
    "        # predict for each cluster\n",
    "        for i , grp in enumerate(x_test_grouped.groups.keys()):\n",
    "            \n",
    "            # get features, death time and event indicator\n",
    "            features = x_test_grouped.get_group(grp)\n",
    "\n",
    "            # get death time and event indicator\n",
    "            y_test_dur_ , y_test_event_ = get_target(features)\n",
    "\n",
    "            # add to lists (useful for mc iterations)\n",
    "            y_test_dur.append(y_test_dur_)\n",
    "            y_test_event.append(y_test_event_)\n",
    "\n",
    "            # curtail features matrix\n",
    "            features = features.iloc[: , :-2].to_numpy()\n",
    "\n",
    "            survival = []\n",
    "\n",
    "            # apply Monte Carlo dropout\n",
    "            for _ in range(mc_iter):\n",
    "                net = self.nets[i][0]\n",
    "                \n",
    "                # Prime dropout layers\n",
    "                net.train()\n",
    "                \n",
    "                # predict\n",
    "                mc_haz = torch.sigmoid(net(features))\n",
    "                mc_survival = torch.cumprod(1 - mc_haz , dim = 1).detach().numpy()\n",
    "\n",
    "                # append survivals from different runs\n",
    "                survival.append(mc_survival)\n",
    "            \n",
    "            # convert to 3d array\n",
    "            survival = np.array(survival)\n",
    "            \n",
    "            surv.append(survival)\n",
    "\n",
    "        # Concatenate durations and event indicators\n",
    "        y_test_dur = np.concatenate(y_test_dur , axis = 0)\n",
    "        y_test_event = np.concatenate(y_test_event , axis = 0)\n",
    "\n",
    "        # QCs\n",
    "        assert len(surv) == self._clusters , 'surv matrix not matching number of clusters'\n",
    "        assert len(x_test) == np.sum([surv[k].shape[1] for k in range(self._clusters)]) , 'test set count mismatch'\n",
    "\n",
    "        mean_ = np.concatenate([surv[k].mean(axis = 0) for k in range(self._clusters)])\n",
    "        up_ = np.concatenate([np.quantile(surv[k] , axis = 0 , q = conf_int_upper) for k in range(self._clusters)])\n",
    "        low_ = np.concatenate([np.quantile(surv[k] , axis = 0 , q = conf_int_lower) for k in range(self._clusters)])\n",
    "        \n",
    "        # QCs\n",
    "        assert mean_.shape[0] == up_.shape[0] == low_.shape[0] == y_test_dur.shape[0] == y_test_event.shape[0] , 'shape mismatch'\n",
    "\n",
    "        # change\n",
    "        self.predicted = True\n",
    "        \n",
    "        return mean_ , up_ , low_ , y_test_dur , y_test_event\n",
    "\n",
    "    def visualize(self , mean_ , low_ , up_ , _from , _to):\n",
    "        '''\n",
    "        visualize the predictions\n",
    "        '''\n",
    "        # get features, death time and event indicator\n",
    "        features = self.test_data\n",
    "\n",
    "        # get death time and event indicator\n",
    "        y_test_dur_ , y_test_event_ = get_target(features)\n",
    "\n",
    "        t_test = Transforms(durations = y_test_dur_)\n",
    "        dur_idx_test = t_test.discrete_transform(_cuts = self.q_cuts)\n",
    "\n",
    "        # get transparency for graph\n",
    "        transparency = self.configs['time_invariant']['test_viz']['transparency']\n",
    "        _ = plot_with_cf(t_test.bin_edges, mean_ , low_ , up_ , _from , _to , transparency = transparency)\n",
    "\n",
    "    def evaluation(self , mean_ , y_test_dur , y_test_event , plot = False):\n",
    "        '''\n",
    "        Evaluation by\n",
    "        1. td c-index\n",
    "        2. Brier score\n",
    "        3. IBS\n",
    "        '''\n",
    "        time_grid_div = self.configs['time_invariant']['eval']['time_grid_div']\n",
    "        time_grid = np.linspace(y_test_dur.min(), y_test_dur.max(), time_grid_div)\n",
    "        \n",
    "        # Evaluation\n",
    "        ev_ = EvalSurv(pd.DataFrame(mean_.T) , y_test_dur , y_test_event , censor_surv='km')\n",
    "        \n",
    "        # brier score\n",
    "        if plot:\n",
    "            ev_.brier_score(time_grid).plot()\n",
    "            plt.ylabel('Brier score')\n",
    "            _ = plt.xlabel('Time')\n",
    "\n",
    "        # td c-index\n",
    "        tdci = ev_.concordance_td()\n",
    "        # print(f'concordance-td: {tdci}')\n",
    "        \n",
    "        # IBS\n",
    "        ibs = ev_.integrated_brier_score(time_grid)\n",
    "        # print(f'integrated brier score {ibs}')\n",
    "        \n",
    "        return tdci , ibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the pickled DataFrames\n",
    "with open('../05_preprocessing_emr_data/data/x_train.pickle', 'rb') as file:\n",
    "    x_train = pickle.load(file)\n",
    "with open('../05_preprocessing_emr_data/data/x_test.pickle', 'rb') as file:\n",
    "    x_test = pickle.load(file)\n",
    "with open('../05_preprocessing_emr_data/data/x_val.pickle', 'rb') as file:\n",
    "    x_val = pickle.load(file)\n",
    "\n",
    "# Read the pickled DataFrame\n",
    "with open('../05_preprocessing_emr_data/data/consolidated_pat_tbl.pickle', 'rb') as file:\n",
    "    consolidated_pat_tbl = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Time_Invariant_Survival(\n",
    "    configs = configs, \n",
    "    train_data = x_train,\n",
    "    test_data = x_test, \n",
    "    val_data = x_val\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training cluster 0\n",
      "Epoch 0: Training Loss: 0.0272754, Val Loss: 0.0285383\n",
      "Epoch 50: Training Loss: 0.0023703, Val Loss: 0.0051537\n",
      "Epoch 100: Training Loss: 0.0028547, Val Loss: 0.0001296\n",
      "Epoch 150: Training Loss: -0.0034460, Val Loss: 0.0038987\n",
      "Epoch 200: Training Loss: 0.0039566, Val Loss: 0.0012336\n",
      "Epoch 250: Training Loss: -0.0037628, Val Loss: 0.0017310\n",
      "Epoch 300: Training Loss: 0.0021255, Val Loss: 0.0015744\n",
      "Epoch 350: Training Loss: 0.0041944, Val Loss: 0.0018822\n",
      "Epoch 400: Training Loss: -0.0003827, Val Loss: 0.0010487\n",
      "Epoch 450: Training Loss: -0.0000230, Val Loss: 0.0012048\n",
      "shapes : (1060, 1060, 1060, 1060)\n",
      "CPU times: total: 4min 56s\n",
      "Wall time: 47.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "obj.fit(verbose = True)\n",
    "mean_ , up_ , low_ , y_test_dur , y_test_event = obj.predict()\n",
    "# obj.visualize(mean_ , up_ , low_ , _from = 40 , _to = 50 )\n",
    "cindex , ibs = obj.evaluation(mean_ , y_test_dur , y_test_event, plot = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # def discretize_test(self):\n",
    "    #     '''\n",
    "    #     function to discretize test and validation survival times\n",
    "    #     '''     \n",
    "    #     # Prepare val and test data\n",
    "    #     get_target = lambda df: (df['time_to_event'].values, df['death'].values)\n",
    "        \n",
    "    #     self.y_test_dur , self.y_test_event = get_target(self.test_data)\n",
    "    #     # self.y_val_dur , self.y_val_event = get_target(self.val_data)\n",
    "\n",
    "    #     t_test = Transforms(durations = self.y_test_dur)\n",
    "    #     self.dur_idx_test = t_test.discrete_transform(_cuts = self.q_cuts)\n",
    "\n",
    "    #     # t_val = Transforms(durations = self.y_val_dur)\n",
    "    #     # self.dur_idx_val = t_val.discrete_transform(_cuts = self.q_cuts)\n",
    "\n",
    "    #     # # prepare validation data with survival matrix (reqd for L1 (NLL Loss))\n",
    "    #     # sm_val = Surv_Matrix(duration_index = self.dur_idx_val , events = self.y_val_event , q_cuts = self.q_cuts)\n",
    "    #     # self.surv_mat_val = sm_val.make_survival_matrix()\n",
    "\n",
    "    #     # change state\n",
    "    #     self.discretized = True\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
