{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "import seaborn as sns\n",
    "\n",
    "# Read the pickled DataFrame\n",
    "with open('data/consolidated_pat_tbl_tv.pickle', 'rb') as file:\n",
    "    consolidated_pat_tbl = pickle.load(file)\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5904"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consolidated_pat_tbl.subject_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncode race (ethnicity)!\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "race_enc = encoder.fit_transform(consolidated_pat_tbl[['race']])\n",
    "mod_df = consolidated_pat_tbl.drop('race' , axis = 1)\n",
    "mod_df = pd.concat([mod_df , pd.DataFrame(\n",
    "    race_enc , \n",
    "    columns = consolidated_pat_tbl[['race']].drop_duplicates().sort_values('race').values.T[0]\n",
    "    )\n",
    "] , axis = 1)\n",
    "\n",
    "# OneHotEncode gender (sex)\n",
    "gender_encoder = OneHotEncoder(sparse_output=False)\n",
    "gender_enc = gender_encoder.fit_transform(mod_df[['gender']])\n",
    "mod_df = mod_df.drop('gender' , axis = 1)\n",
    "mod_df = pd.concat([mod_df , pd.DataFrame(\n",
    "    gender_enc , \n",
    "    columns = consolidated_pat_tbl[['gender']].drop_duplicates().sort_values('gender').values.T[0]\n",
    "    )\n",
    "] , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_splitter_tv(df , test_size = 0.2 , val_size = 0.2 , patient_col = 'subject_id'):\n",
    "    # get all patients\n",
    "    pats = df[patient_col].unique()\n",
    "    # inplace shuffle\n",
    "    np.random.shuffle(pats)\n",
    "\n",
    "    # get splits\n",
    "    test_pats = pats[:int(test_size*len(pats))]\n",
    "    val_pats = pats[int(test_size*len(pats)):int(test_size*len(pats))+int(val_size*len(pats))]\n",
    "    train_pats = pats[int(test_size*len(pats))+int(val_size*len(pats)):]\n",
    "\n",
    "    # allocate\n",
    "    df_test = df[df['subject_id'].isin(test_pats)]\n",
    "    df_val = df[df['subject_id'].isin(val_pats)]\n",
    "    df_train = df[df['subject_id'].isin(train_pats)]\n",
    "    \n",
    "    # check\n",
    "    assert df_train.subject_id.nunique() + df_test.subject_id.nunique() + df_val.subject_id.nunique() == df.subject_id.nunique()\n",
    "    return df_train , df_test , df_val\n",
    "\n",
    "df_train , df_test , df_val = train_test_splitter_tv(mod_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def impute(df):\n",
    "#     _columns = df.columns.astype('str')\n",
    "#     df.columns = _columns\n",
    "\n",
    "#     # Imputation\n",
    "#     imputer = SimpleImputer(strategy = 'most_frequent')\n",
    "#     df = pd.DataFrame(imputer.fit_transform(df) , columns = _columns)\n",
    "\n",
    "#     # Check for null\n",
    "#     assert np.round(df.notnull().sum()/len(df)).sum() == df.shape[1]\n",
    "\n",
    "#     # # name change\n",
    "#     # df.rename(columns = {'50907':'cholesterol' , '50983':'sodium' , '51133':'lymphocyte' , '51222':'hemoglobin'} , inplace = True)\n",
    "\n",
    "#     return df\n",
    "\n",
    "# # use on train test val data\n",
    "# df_train = impute(df_train)\n",
    "# df_test = impute(df_test)\n",
    "# df_val = impute(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(df):\n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaled_columns = [\n",
    "    'cholesterol', 'sodium', 'lymphocyte', 'hemoglobin',\n",
    "    'temperature', 'heartrate', 'resprate', 'o2sat', 'sbp', 'dbp','anchor_age','BMI (kg/m2)',\n",
    "    'Height (Inches)', 'Weight (Lbs)'\n",
    "    ]\n",
    "\n",
    "    unscaled_columns = ['acebutolol', 'amlodipine', 'atenolol', 'benazepril', 'candesartan',\n",
    "        'captopril', 'diltiazem', 'felodipine', 'irbesartan', 'lisinopril',\n",
    "        'moexipril', 'nadolol', 'nebivolol', 'nicardipine', 'nifedipine',\n",
    "        'olmesartan', 'propranolol', 'quinapril', 'ramipril', 'telmisartan',\n",
    "        'trandolapril', 'valsartan', 'verapamil', 'ASIAN', 'BLACK', 'HISPANIC', 'NATIVE',\n",
    "        'OTHER', 'WHITE' ,'F','M', 'subject_id','timediff']\n",
    "\n",
    "    scale = [([col], StandardScaler()) for col in scaled_columns]\n",
    "    no_scale = [(col, None) for col in unscaled_columns]\n",
    "\n",
    "    x_mapper = DataFrameMapper(scale + no_scale)\n",
    "    \n",
    "    # scale data\n",
    "    x = pd.DataFrame(x_mapper.fit_transform(df) , \n",
    "                        columns = scaled_columns + unscaled_columns\n",
    "                        )\n",
    "    # add subject_id and timediff separately\n",
    "    # x_ = pd.concat([x , df[['subject_id']]] , axis = 1)\n",
    "    # print(x.shape , x_.shape)\n",
    "    # get targets\n",
    "    y = df[['subject_id','time_to_event','death']].drop_duplicates().drop('subject_id', axis = 1)\n",
    "\n",
    "    # Check\n",
    "    assert x.subject_id.nunique() == len(y) , 'target and feature length mismatch' \n",
    "    assert x.shape[0] == df.shape[0] , 'row mismatch'\n",
    "    return x , y\n",
    "\n",
    "# Scale\n",
    "x_train , y_train = scaling(df_train)\n",
    "x_test , y_test = scaling(df_test)\n",
    "x_val , y_val = scaling(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero_var_cols ['candesartan', 'nebivolol']\n"
     ]
    }
   ],
   "source": [
    "def check_var(df):\n",
    "    zero_var_cols = []\n",
    "    # Check which columns have zero variance\n",
    "    for _col in df.columns[:-2]:\n",
    "        var = df[_col].var()\n",
    "        if var == 0:\n",
    "            zero_var_cols.append(_col)\n",
    "    return zero_var_cols\n",
    "\n",
    "# get rid of columns that have zero variance in training data as they won't add anything to the training\n",
    "zero_var_cols = check_var(x_train)\n",
    "print(f'zero_var_cols {zero_var_cols}')\n",
    "\n",
    "# Drop the cols\n",
    "x_train.drop(zero_var_cols, axis = 1, inplace = True)\n",
    "x_test.drop(zero_var_cols, axis = 1, inplace = True)\n",
    "x_val.drop(zero_var_cols, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_masking(df):\n",
    "    '''\n",
    "    create a null mask\n",
    "    '''\n",
    "    mask = df.isnull().astype('int')\n",
    "    mask.columns = [col+'_mask' for col in df.isnull().astype('int').columns]\n",
    "    mask = mask.iloc[: , :-2]\n",
    "    return mask\n",
    "\n",
    "# Get masks\n",
    "mask_train = null_masking(x_train)\n",
    "mask_test = null_masking(x_test)\n",
    "mask_val = null_masking(x_val)\n",
    "\n",
    "# Add masks\n",
    "_x_train = pd.concat([x_train , mask_train], axis = 1)\n",
    "_x_test = pd.concat([x_test , mask_test], axis = 1)\n",
    "_x_val = pd.concat([x_val , mask_val], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_reshape(df , subject_col):\n",
    "    pats = list(df[subject_col].unique())\n",
    "\n",
    "    df_reshape = []\n",
    "\n",
    "    # selecting individual patients in the df\n",
    "    for pat in pats:\n",
    "        pat_df = df[df[subject_col]==pat].fillna(method = 'ffill').fillna(method = 'bfill') # first, forward fill\n",
    "        # then back fill for any remaining nulls\n",
    "        df_reshape.append(pat_df.drop(subject_col , axis = 1).to_numpy())\n",
    "\n",
    "    return np.array(df_reshape, dtype = 'object')\n",
    "\n",
    "# Create variable length 3D representations\n",
    "x_train_reshape = feature_reshape(_x_train , 'subject_id')\n",
    "x_test_reshape = feature_reshape(_x_test , 'subject_id')\n",
    "x_val_reshape = feature_reshape(_x_val , 'subject_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the DataFrame\n",
    "with open('data/x_train_reshape_tv.pickle', 'wb') as file:\n",
    "    pickle.dump(x_train_reshape, file)\n",
    "\n",
    "with open('data/x_test_reshape_tv.pickle', 'wb') as file:\n",
    "    pickle.dump(x_test_reshape, file)\n",
    "\n",
    "with open('data/x_val_reshape_tv.pickle', 'wb') as file:\n",
    "    pickle.dump(x_val_reshape, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def feature_reshape(df, group_column = 0):\n",
    "#     '''\n",
    "#     Convert to 3D array\n",
    "#     '''\n",
    "#     # convert to numpy\n",
    "#     df_arr = df.to_numpy()\n",
    "\n",
    "#     # Step 1: Sort the array based on the group_column\n",
    "#     sorted_data = df_arr[df_arr[:, group_column].argsort()]\n",
    "\n",
    "#     # Step 2: Find unique values in the group_column\n",
    "#     group_values, group_counts = np.unique(sorted_data[:, group_column], return_counts=True)\n",
    "\n",
    "#     # Step 3: Use np.split() to split the sorted_data into separate arrays based on the unique values\n",
    "#     grouped_data = np.split(sorted_data, np.cumsum(group_counts)[:-1])\n",
    "\n",
    "#     # Step 4: Reshape the resulting arrays into a 3D matrix\n",
    "#     result = np.array(grouped_data , dtype = 'object')\n",
    "\n",
    "#     result_new = []\n",
    "\n",
    "#     # # remove subject id\n",
    "#     # for pat_img in result:\n",
    "#     #     pat_img = pat_img[: , :-1]\n",
    "#     #     result_new.append(pat_img)\n",
    "    \n",
    "#     # return np.array(result_new , dtype = 'object')\n",
    "#     return np.array(result , dtype = 'object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create variable length 3D representations\n",
    "# x_train_reshape = feature_reshape(x_train , group_column = -1)\n",
    "# x_test_reshape = feature_reshape(x_test , group_column = -1)\n",
    "# x_val_reshape = feature_reshape(x_val , group_column = -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
