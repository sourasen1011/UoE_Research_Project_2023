{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "import seaborn as sns\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "# Read the pickled DataFrame\n",
    "with open('data/consolidated_pat_tbl_tv.pickle', 'rb') as file:\n",
    "    consolidated_pat_tbl = pickle.load(file)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncode race (ethnicity)!\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "race_enc = encoder.fit_transform(consolidated_pat_tbl[['race']])\n",
    "mod_df = consolidated_pat_tbl.drop('race' , axis = 1)\n",
    "mod_df = pd.concat([mod_df , pd.DataFrame(\n",
    "    race_enc , \n",
    "    columns = consolidated_pat_tbl[['race']].drop_duplicates().sort_values('race').values.T[0]\n",
    "    )\n",
    "] , axis = 1)\n",
    "\n",
    "# Leave out one of the one-hot encoded columns so as to not raise multicollinearity issues\n",
    "mod_df.drop('OTHER' , axis = 1 , inplace = True)\n",
    "\n",
    "# OneHotEncode gender (sex)\n",
    "gender_encoder = OneHotEncoder(sparse_output=False)\n",
    "gender_enc = gender_encoder.fit_transform(mod_df[['gender']])\n",
    "mod_df = mod_df.drop('gender' , axis = 1)\n",
    "mod_df = pd.concat([mod_df , pd.DataFrame(\n",
    "    gender_enc , \n",
    "    columns = consolidated_pat_tbl[['gender']].drop_duplicates().sort_values('gender').values.T[0]\n",
    "    )\n",
    "] , axis = 1)\n",
    "\n",
    "# Leave out one of the one-hot encoded columns so as to not raise multicollinearity issues\n",
    "mod_df.drop('F' , axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_splitter_tv(df , test_size = 0.2 , val_size = 0.2 , patient_col = 'subject_id'):\n",
    "    # get all patients\n",
    "    pats = df[patient_col].unique()\n",
    "    # inplace shuffle\n",
    "    np.random.shuffle(pats)\n",
    "\n",
    "    # get splits\n",
    "    test_pats = pats[:int(test_size*len(pats))]\n",
    "    val_pats = pats[int(test_size*len(pats)):int(test_size*len(pats))+int(val_size*len(pats))]\n",
    "    train_pats = pats[int(test_size*len(pats))+int(val_size*len(pats)):]\n",
    "\n",
    "    # allocate\n",
    "    df_test = df[df['subject_id'].isin(test_pats)]\n",
    "    df_val = df[df['subject_id'].isin(val_pats)]\n",
    "    df_train = df[df['subject_id'].isin(train_pats)]\n",
    "    \n",
    "    # check\n",
    "    assert df_train.subject_id.nunique() + df_test.subject_id.nunique() + df_val.subject_id.nunique() == df.subject_id.nunique()\n",
    "    return df_train , df_test , df_val\n",
    "\n",
    "df_train , df_test , df_val = train_test_splitter_tv(mod_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_target(df_original , subject_col , time_2_eve_col , event_col , timediff_col):\n",
    "    df = df_original.copy()\n",
    "\n",
    "    # cumulative time difference!\n",
    "    df[timediff_col] = df.groupby(subject_col)[timediff_col].cumsum()\n",
    "\n",
    "    target_cols = [time_2_eve_col , event_col]\n",
    "\n",
    "    # feature cols\n",
    "    x = df[[col for col in df.columns if col not in target_cols]]\n",
    "\n",
    "    # get targets\n",
    "    y = df[[subject_col]+target_cols].drop_duplicates().drop(subject_col, axis = 1)\n",
    "\n",
    "    # Check\n",
    "    assert x[subject_col].nunique() == len(y) , 'target and feature length mismatch' \n",
    "    assert x.shape[0] == df.shape[0] , 'row mismatch'\n",
    "    return x , y\n",
    "\n",
    "x_train , y_train = get_feature_target(df_train , 'subject_id' , 'time_to_event' , 'death' , 'timediff')\n",
    "x_test , y_test = get_feature_target(df_test , 'subject_id' , 'time_to_event' , 'death' , 'timediff')\n",
    "x_val , y_val = get_feature_target(df_val , 'subject_id' , 'time_to_event' , 'death' , 'timediff')\n",
    "\n",
    "# Check if cumulative time diff col has been properly added\n",
    "assert x_train.groupby('subject_id').agg({'timediff':'min'}).sum()[0] == 0 # only checking for train data should be enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero_var_cols ['acebutolol', 'telmisartan']\n"
     ]
    }
   ],
   "source": [
    "def check_var(df):\n",
    "    zero_var_cols = []\n",
    "    # Check which columns have zero variance\n",
    "    for _col in df.columns[:-2]:\n",
    "        var = df[_col].var()\n",
    "        if var == 0:\n",
    "            zero_var_cols.append(_col)\n",
    "    return zero_var_cols\n",
    "\n",
    "# get rid of columns that have zero variance in training data as they won't add anything to the training\n",
    "zero_var_cols = check_var(x_train)\n",
    "print(f'zero_var_cols {zero_var_cols}')\n",
    "\n",
    "# Drop the cols\n",
    "_x_train = x_train.drop(zero_var_cols, axis = 1)\n",
    "_x_test = x_test.drop(zero_var_cols, axis = 1)\n",
    "_x_val = x_val.drop(zero_var_cols, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_masking(df,subject_col):\n",
    "    '''\n",
    "    create a null mask\n",
    "    '''\n",
    "    # null_cols = list(df.isnull().sum()[df.isnull().sum()!=0].index)\n",
    "    mask = df.isnull().astype('int')\n",
    "    mask.drop(subject_col , axis = 1 , inplace = True)\n",
    "    # add a suffix\n",
    "    mask.columns = [col+'_mask' for col in mask.columns]\n",
    "    return mask\n",
    "\n",
    "# Get masks\n",
    "mask_train = null_masking(x_train,'subject_id')\n",
    "mask_test = null_masking(x_test,'subject_id')\n",
    "mask_val = null_masking(x_val,'subject_id')\n",
    "\n",
    "# Add masks\n",
    "_x_train = pd.concat([mask_train, _x_train], axis = 1)\n",
    "_x_test = pd.concat([mask_test, _x_test], axis = 1)\n",
    "_x_val = pd.concat([mask_val, _x_val], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 47s\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def feature_impute_and_reshape(df , subject_col , timediff_col , divisions):\n",
    "    pats = list(df[subject_col].unique())\n",
    "\n",
    "    # init container\n",
    "    df_reshape = []\n",
    "\n",
    "    # selecting individual patients in the df\n",
    "    for pat in pats:\n",
    "        pat_df = df[df[subject_col]==pat].fillna(method = 'ffill') # forward fill\n",
    "        \n",
    "        # Further Imputation\n",
    "        imputer = SimpleImputer(strategy = 'most_frequent', keep_empty_features = True)\n",
    "        imputed_pat_df = imputer.fit_transform(pat_df)\n",
    "        _pat_df = pd.DataFrame(imputed_pat_df , columns = pat_df.columns)\n",
    "        \n",
    "        # quantized time difference\n",
    "        timediff_div = np.linspace(\n",
    "            _pat_df[timediff_col].min() , \n",
    "            _pat_df[timediff_col].max() ,\n",
    "            divisions\n",
    "        )\n",
    "\n",
    "        # matrix to store patient features at timestamp-wise divisions of their history\n",
    "        mat = []\n",
    "\n",
    "        # interpolate\n",
    "        _cols = [col for col in _pat_df.columns if col!='subject_id' and col!='timediff']\n",
    "        for col in _cols:\n",
    "            f = interp1d(_pat_df[timediff_col] , _pat_df[col]) # fit\n",
    "            col_div = f(timediff_div)\n",
    "            mat.append(col_div)\n",
    "            \n",
    "        mat = np.column_stack(mat)\n",
    "\n",
    "        # add to container\n",
    "        df_reshape.append(mat)\n",
    "\n",
    "    arr = np.array(df_reshape, dtype = 'object').astype('float')\n",
    "\n",
    "    # To normalize along the columns (dimension 1), you can do the following:\n",
    "    norms = np.linalg.norm(arr, ord=2, axis=1)  # Compute L2-norm along axis 1\n",
    "\n",
    "    # To avoid division by zero, add a small epsilon (e.g., 1e-8) to the norms\n",
    "    epsilon = 1e-8\n",
    "    normalized_array = arr / (norms[:, np.newaxis] + epsilon)\n",
    "\n",
    "    normalized_array = torch.Tensor(\n",
    "        normalized_array.reshape(\n",
    "            normalized_array.shape[0] ,\n",
    "            1 ,\n",
    "            normalized_array.shape[1] , \n",
    "            normalized_array.shape[2]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return normalized_array\n",
    "\n",
    "# Create variable length 3D representations\n",
    "x_train_reshape = feature_impute_and_reshape(\n",
    "    _x_train , \n",
    "    subject_col = 'subject_id' , \n",
    "    timediff_col = 'timediff' , \n",
    "    divisions = 10\n",
    ")\n",
    "\n",
    "x_test_reshape = feature_impute_and_reshape(\n",
    "    _x_test , \n",
    "    subject_col = 'subject_id' , \n",
    "    timediff_col = 'timediff' , \n",
    "    divisions = 10\n",
    ")\n",
    "\n",
    "x_val_reshape = feature_impute_and_reshape(\n",
    "    _x_val , \n",
    "    subject_col = 'subject_id' , \n",
    "    timediff_col = 'timediff' , \n",
    "    divisions = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_check(arr):\n",
    "    for i in range(len(arr)):\n",
    "        s = torch.isnan(arr[i]).sum().sum()\n",
    "        if s > 0:\n",
    "            print(s)\n",
    "\n",
    "# Apply\n",
    "nan_check(x_train_reshape)\n",
    "nan_check(x_test_reshape)\n",
    "nan_check(x_val_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the DataFrame\n",
    "with open('data/x_train_reshape_tv.pickle', 'wb') as file:\n",
    "    pickle.dump(x_train_reshape, file)\n",
    "\n",
    "with open('data/x_test_reshape_tv.pickle', 'wb') as file:\n",
    "    pickle.dump(x_test_reshape, file)\n",
    "\n",
    "with open('data/x_val_reshape_tv.pickle', 'wb') as file:\n",
    "    pickle.dump(x_val_reshape, file)\n",
    "\n",
    "# Pickle the targets\n",
    "with open('data/y_train.pickle', 'wb') as file:\n",
    "    pickle.dump(y_train, file)\n",
    "\n",
    "with open('data/y_test.pickle', 'wb') as file:\n",
    "    pickle.dump(y_test, file)\n",
    "\n",
    "with open('data/y_val.pickle', 'wb') as file:\n",
    "    pickle.dump(y_val, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **JUNK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def feature_reshape(df, group_column = 0):\n",
    "#     '''\n",
    "#     Convert to 3D array\n",
    "#     '''\n",
    "#     # convert to numpy\n",
    "#     df_arr = df.to_numpy()\n",
    "\n",
    "#     # Step 1: Sort the array based on the group_column\n",
    "#     sorted_data = df_arr[df_arr[:, group_column].argsort()]\n",
    "\n",
    "#     # Step 2: Find unique values in the group_column\n",
    "#     group_values, group_counts = np.unique(sorted_data[:, group_column], return_counts=True)\n",
    "\n",
    "#     # Step 3: Use np.split() to split the sorted_data into separate arrays based on the unique values\n",
    "#     grouped_data = np.split(sorted_data, np.cumsum(group_counts)[:-1])\n",
    "\n",
    "#     # Step 4: Reshape the resulting arrays into a 3D matrix\n",
    "#     result = np.array(grouped_data , dtype = 'object')\n",
    "\n",
    "#     result_new = []\n",
    "\n",
    "#     # # remove subject id\n",
    "#     # for pat_img in result:\n",
    "#     #     pat_img = pat_img[: , :-1]\n",
    "#     #     result_new.append(pat_img)\n",
    "    \n",
    "#     # return np.array(result_new , dtype = 'object')\n",
    "#     return np.array(result , dtype = 'object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create variable length 3D representations\n",
    "# x_train_reshape = feature_reshape(x_train , group_column = -1)\n",
    "# x_test_reshape = feature_reshape(x_test , group_column = -1)\n",
    "# x_val_reshape = feature_reshape(x_val , group_column = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scaling(df):\n",
    "#     # Scaling\n",
    "#     scaler = StandardScaler()\n",
    "#     scaled_columns = [\n",
    "#     'cholesterol', 'sodium', 'lymphocyte', 'hemoglobin',\n",
    "#     'temperature', 'heartrate', 'resprate', 'o2sat', 'sbp', 'dbp','anchor_age','BMI (kg/m2)',\n",
    "#     'Height (Inches)', 'Weight (Lbs)'\n",
    "#     ]\n",
    "\n",
    "#     unscaled_columns = ['acebutolol', 'amlodipine', 'atenolol', 'benazepril', 'candesartan',\n",
    "#         'captopril', 'diltiazem', 'felodipine', 'irbesartan', 'lisinopril',\n",
    "#         'moexipril', 'nadolol', 'nebivolol', 'nicardipine', 'nifedipine',\n",
    "#         'olmesartan', 'propranolol', 'quinapril', 'ramipril', 'telmisartan',\n",
    "#         'trandolapril', 'valsartan', 'verapamil', 'ASIAN', 'BLACK', 'HISPANIC', 'NATIVE',\n",
    "#         'OTHER', 'WHITE' ,'F','M', 'subject_id','timediff']\n",
    "\n",
    "#     # scale = [([col], StandardScaler()) for col in scaled_columns]\n",
    "#     scale = [([col], None) for col in scaled_columns]\n",
    "#     no_scale = [(col, None) for col in unscaled_columns]\n",
    "\n",
    "#     x_mapper = DataFrameMapper(scale + no_scale)\n",
    "    \n",
    "#     # scale data\n",
    "#     x = pd.DataFrame(x_mapper.fit_transform(df) , \n",
    "#                         columns = scaled_columns + unscaled_columns\n",
    "#                         )\n",
    "#     # add subject_id and timediff separately\n",
    "    \n",
    "#     # get targets\n",
    "#     y = df[['subject_id','time_to_event','death']].drop_duplicates().drop('subject_id', axis = 1)\n",
    "\n",
    "#     # Check\n",
    "#     assert x.subject_id.nunique() == len(y) , 'target and feature length mismatch' \n",
    "#     assert x.shape[0] == df.shape[0] , 'row mismatch'\n",
    "#     return x , y\n",
    "\n",
    "# # Scale\n",
    "# x_train , y_train = scaling(df_train)\n",
    "# x_test , y_test = scaling(df_test)\n",
    "# x_val , y_val = scaling(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def null_masking(df):\n",
    "# #     '''\n",
    "# #     create a null mask\n",
    "# #     '''\n",
    "# #     mask = df.isnull().astype('int')\n",
    "# #     mask.columns = [col+'_mask' for col in df.isnull().astype('int').columns]\n",
    "# #     mask = mask.iloc[: , :-2]\n",
    "# #     return mask\n",
    "\n",
    "# # # Get masks\n",
    "# # mask_train = null_masking(x_train)\n",
    "# # mask_test = null_masking(x_test)\n",
    "# # mask_val = null_masking(x_val)\n",
    "\n",
    "# # # Add masks\n",
    "# # _x_train = pd.concat([mask_train, x_train], axis = 1)\n",
    "# # _x_test = pd.concat([mask_test, x_test], axis = 1)\n",
    "# # _x_val = pd.concat([mask_val, x_val], axis = 1)\n",
    "\n",
    "# _x_train = x_train\n",
    "# _x_test = x_test\n",
    "# _x_val = x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = x_train_reshape.astype('float')\n",
    "# input = torch.Tensor(input.reshape(input.shape[0] , 1 , input.shape[1] , input.shape[2]))\n",
    "# print(input.shape)\n",
    "# c2d = nn.Conv2d(1 , 14 , kernel_size = 3)\n",
    "# # c2d(torch.Tensor(input))\n",
    "# enc = Encoder()\n",
    "# dec = Decoder()\n",
    "# dec(enc(input)).shape , input.shape\n",
    "\n",
    "# def create_distance_matrix(matrix_array):\n",
    "#     distance_mtx = []\n",
    "    \n",
    "#     for i in range(matrix_array.shape[0]):\n",
    "#         ssd = list(np.sum((matrix_array - matrix_array[i])**2 , axis = (1,2)))\n",
    "#         frob = np.sqrt(ssd)\n",
    "#         distance_mtx.append(frob)\n",
    "    \n",
    "#     return np.column_stack(distance_mtx)\n",
    "\n",
    "# dm = create_distance_matrix(x_train_reshape)\n",
    "# dm.shape\n",
    "\n",
    "# import numpy as np\n",
    "# from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Step 1: Prepare the distance matrix\n",
    "# # Replace this with your actual distance matrix\n",
    "# distance_matrix = dm\n",
    "\n",
    "# # Step 2: Choose the linkage method\n",
    "# linkage_method = 'average'  # You can try 'single' or 'complete' as well\n",
    "\n",
    "# # Step 3: Perform hierarchical clustering\n",
    "# Z = linkage(distance_matrix, method=linkage_method)\n",
    "\n",
    "# # Step 4: Determine the number of clusters using dendrogram\n",
    "# dendrogram(Z)\n",
    "# plt.xlabel('Data points')\n",
    "# plt.ylabel('Distance')\n",
    "# plt.title('Dendrogram')\n",
    "# plt.show()\n",
    "\n",
    "# # Step 5: Cut the dendrogram to get clusters\n",
    "# num_clusters = 7 # You can adjust this based on the dendrogram visualization\n",
    "\n",
    "# # Step 6: Assign data points to clusters\n",
    "# clusters = fcluster(Z, num_clusters, criterion='maxclust')\n",
    "# print(\"Data point clustering:\", clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled_cols = ['temperature', 'heartrate', 'resprate', 'o2sat', 'sbp', 'dbp', 'cholesterol', 'sodium', \n",
    "#                 'lymphocyte', 'hemoglobin','BMI (kg/m2)','Height (Inches)', 'Weight (Lbs)','timediff']\n",
    "\n",
    "# x_train_prp = feature_preprocess(\n",
    "#     _x_train , \n",
    "#     subject_col = 'subject_id' , \n",
    "#     scaled_cols = scaled_cols ,\n",
    "#     scaling_func = scaler_func ,\n",
    "#     filling_func = fill_to_max_time , \n",
    "#     max_time = max_time\n",
    "# )\n",
    "# x_test_prp = feature_preprocess(\n",
    "#     _x_test , \n",
    "#     subject_col = 'subject_id' , \n",
    "#     scaled_cols = scaled_cols ,\n",
    "#     scaling_func = scaler_func ,\n",
    "#     filling_func = fill_to_max_time , \n",
    "#     max_time = max_time\n",
    "# )\n",
    "# x_val_prp = feature_preprocess(\n",
    "#     _x_val , \n",
    "#     subject_col = 'subject_id' , \n",
    "#     scaled_cols = scaled_cols ,\n",
    "#     scaling_func = scaler_func ,\n",
    "#     filling_func = fill_to_max_time , \n",
    "#     max_time = max_time\n",
    "# )\n",
    "\n",
    "# def scaler_func(df , scaled_cols):\n",
    "#     unscaled_cols = [col for col in _x_train.columns if col not in scaled_cols]\n",
    "\n",
    "#     # Create a StandardScaler object\n",
    "#     my_scaler = StandardScaler()\n",
    "\n",
    "#     # Fit the scaler to your data and then transform the data\n",
    "#     scaled_data = pd.DataFrame(my_scaler.fit_transform(df[scaled_cols]) , columns = scaled_cols)\n",
    "#     unscaled_data = df[unscaled_cols].reset_index().drop('index' , axis = 1)\n",
    "#     assert scaled_data.shape[0] == unscaled_data.shape[0]\n",
    "#     df_scaled = pd.concat([unscaled_data , scaled_data] , axis = 1)\n",
    "\n",
    "#     return df_scaled\n",
    "    \n",
    "# def fill_to_max_time(original_tensor, max_time):\n",
    "#     # Create a new zero-filled tensor of shape (1000, 20)\n",
    "#     desired_shape = (max_time, original_tensor.shape[1])\n",
    "#     new_tensor = torch.zeros(desired_shape)\n",
    "\n",
    "#     # Copy the original data into the new tensor up to the original number of rows\n",
    "#     new_tensor[:original_tensor.shape[0], :] = original_tensor\n",
    "\n",
    "#     return new_tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
