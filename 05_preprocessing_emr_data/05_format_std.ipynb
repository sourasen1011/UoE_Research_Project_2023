{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "# Read the pickled DataFrame\n",
    "with open('consolidated_pat_tbl.pickle', 'rb') as file:\n",
    "    consolidated_pat_tbl = pickle.load(file)\n",
    "\n",
    "# QC\n",
    "pd.get_dummies(consolidated_pat_tbl['race']).sum().sum() == consolidated_pat_tbl['subject_id'].nunique()\n",
    "\n",
    "# Cast as integer\n",
    "for col in consolidated_pat_tbl.columns:\n",
    "    if col != 'race':\n",
    "        consolidated_pat_tbl[col] = consolidated_pat_tbl[col].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncode race!\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "race_enc = encoder.fit_transform(consolidated_pat_tbl[['race']])\n",
    "mod_df = consolidated_pat_tbl.drop('race' , axis = 1)\n",
    "mod_df = pd.concat([mod_df , pd.DataFrame(race_enc)] , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_splitter(df , test_size = 0.2 , val_size = 0.2 , duration_col = 'futime' , event_col = 'death'):\n",
    "    df_test = df.sample(frac=test_size)\n",
    "    df_train = df.drop(df_test.index)\n",
    "\n",
    "    df_val = df_train.sample(frac=val_size)\n",
    "    df_train = df_train.drop(df_val.index)\n",
    "\n",
    "    return df_train , df_val , df_test\n",
    "\n",
    "df_train , df_test , df_val = train_test_splitter(mod_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_stuff(df):\n",
    "    _columns = df.columns.astype('str')\n",
    "    df.columns = _columns\n",
    "\n",
    "    # Imputation\n",
    "    imputer = SimpleImputer(strategy = 'most_frequent')\n",
    "    df = pd.DataFrame(imputer.fit_transform(df) , columns = _columns)\n",
    "\n",
    "    # Check for null\n",
    "    assert np.round(df.notnull().sum()/len(df)).sum() == df.shape[1]\n",
    "\n",
    "    # name change\n",
    "    df.rename(columns = {'50907':'cholesterol' , '50983':'sodium' , '51133':'lymphocyte' , '51222':'hemoglobin'} , inplace = True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# use on train test val data\n",
    "df_train = _preprocess_stuff(df_train)\n",
    "df_test = _preprocess_stuff(df_test)\n",
    "df_val = _preprocess_stuff(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_columns = [\n",
    "    'cholesterol','sodium','lymphocyte','hemoglobin','temperature','heartrate','resprate',\n",
    "    'o2sat','sbp','dbp','Acebutolol','Amlodipine','Atenolol','Captopril','Diltiazem',\n",
    "    'Felodipine','Lisinopril','Moexipril','NIFEdipine','Nadolol','NiCARdipine','Propranolol',\n",
    "    'Quinapril','Ramipril','Trandolapril','Valsartan','Verapamil','amLODIPine','atenolol',\n",
    "    'benazepril','candesartan','felodipine','irbesartan','nebivolol','olmesartan','ramipril',\n",
    "    'telmisartan','valsartan'\n",
    "    ]\n",
    "\n",
    "unscaled_columns = ['0','1','2','3','4','5','time_to_event','death']\n",
    "\n",
    "scale = [([col], StandardScaler()) for col in scaled_columns]\n",
    "no_scale = [(col, None) for col in unscaled_columns]\n",
    "\n",
    "x_mapper = DataFrameMapper(scale + no_scale)\n",
    "\n",
    "# scale train test val data\n",
    "x_train = pd.DataFrame(x_mapper.fit_transform(df_train).astype('float32') , columns = scaled_columns + unscaled_columns)\n",
    "x_val = pd.DataFrame(x_mapper.transform(df_val).astype('float32') , columns = scaled_columns + unscaled_columns)\n",
    "x_test = pd.DataFrame(x_mapper.transform(df_test).astype('float32') , columns = scaled_columns + unscaled_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the DataFrame\n",
    "with open('x_train.pickle', 'wb') as file:\n",
    "    pickle.dump(x_train, file)\n",
    "\n",
    "with open('x_val.pickle', 'wb') as file:\n",
    "    pickle.dump(x_val, file)\n",
    "\n",
    "with open('x_test.pickle', 'wb') as file:\n",
    "    pickle.dump(x_test, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
