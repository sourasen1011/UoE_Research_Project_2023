{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from losses import nll_logistic_hazard\n",
    "from model import MLP\n",
    "from utils import MyDataset\n",
    "from transforms import Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper \n",
    "import torch # For building the networks \n",
    "import torchtuples as tt # Some useful functions\n",
    "\n",
    "from pycox.datasets import metabric\n",
    "from pycox.models import LogisticHazard\n",
    "# from pycox.models import PMF\n",
    "# from pycox.models import DeepHitSingle\n",
    "from pycox.evaluation import EvalSurv\n",
    "\n",
    "# We also set some seeds to make this reproducable.\n",
    "# Note that on gpu, there is still some randomness.\n",
    "np.random.seed(1234)\n",
    "_ = torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>duration</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.603834</td>\n",
       "      <td>7.811392</td>\n",
       "      <td>10.797988</td>\n",
       "      <td>5.967607</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56.840000</td>\n",
       "      <td>99.333336</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.284882</td>\n",
       "      <td>9.581043</td>\n",
       "      <td>10.204620</td>\n",
       "      <td>5.664970</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85.940002</td>\n",
       "      <td>95.733330</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.654017</td>\n",
       "      <td>5.341846</td>\n",
       "      <td>8.646379</td>\n",
       "      <td>5.655888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.910004</td>\n",
       "      <td>239.300003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.456747</td>\n",
       "      <td>5.339741</td>\n",
       "      <td>10.555724</td>\n",
       "      <td>6.008429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.849998</td>\n",
       "      <td>56.933334</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.425826</td>\n",
       "      <td>6.331182</td>\n",
       "      <td>10.455145</td>\n",
       "      <td>5.749053</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70.519997</td>\n",
       "      <td>123.533333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x0        x1         x2        x3   x4   x5   x6   x7         x8  \\\n",
       "0  5.603834  7.811392  10.797988  5.967607  1.0  1.0  0.0  1.0  56.840000   \n",
       "1  5.284882  9.581043  10.204620  5.664970  1.0  0.0  0.0  1.0  85.940002   \n",
       "3  6.654017  5.341846   8.646379  5.655888  0.0  0.0  0.0  0.0  66.910004   \n",
       "4  5.456747  5.339741  10.555724  6.008429  1.0  0.0  0.0  1.0  67.849998   \n",
       "5  5.425826  6.331182  10.455145  5.749053  1.0  1.0  0.0  1.0  70.519997   \n",
       "\n",
       "     duration  event  \n",
       "0   99.333336      0  \n",
       "1   95.733330      1  \n",
       "3  239.300003      0  \n",
       "4   56.933334      1  \n",
       "5  123.533333      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = metabric.read_df()\n",
    "df_test = df_train.sample(frac=0.2)\n",
    "df_train = df_train.drop(df_test.index)\n",
    "\n",
    "df_val = df_train.sample(frac=0.2)\n",
    "df_train = df_train.drop(df_val.index)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices [3 3 7 ... 1 6 4]\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "train_trans = Transforms()\n",
    "df_train , y_train = train_trans.modify_data(data = df_train , dur_col = 'duration' , eve_col = 'event' , cuts = 10)\n",
    "print(f'indices {train_trans.bucket_indices}')\n",
    "\n",
    "#Val\n",
    "val_trans = Transforms()\n",
    "df_val , y_val = val_trans.modify_data(data = df_val , dur_col = 'duration' , eve_col = 'event' , cuts = 10)\n",
    "# print(f'indices {val_trans.bucket_indices}')\n",
    "\n",
    "#Test\n",
    "test_trans = Transforms()\n",
    "df_test , y_test = test_trans.modify_data(data = df_test , dur_col = 'duration' , eve_col = 'event' , cuts = 10)\n",
    "# print(f'indices {test_trans.bucket_indices}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_standardize = ['x0', 'x1', 'x2', 'x3', 'x8']\n",
    "cols_leave = ['x4', 'x5', 'x6', 'x7']\n",
    "\n",
    "standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "x_mapper = DataFrameMapper(standardize + leave)\n",
    "\n",
    "x_train = x_mapper.fit_transform(df_train).astype('float32')\n",
    "x_val = x_mapper.transform(df_val).astype('float32')\n",
    "x_test = x_mapper.transform(df_test).astype('float32')\n",
    "\n",
    "# num_durations = 10\n",
    "\n",
    "# labtrans = LogisticHazard.label_transform(num_durations)\n",
    "# # labtrans = PMF.label_transform(num_durations)\n",
    "# # labtrans = DeepHitSingle.label_transform(num_durations)\n",
    "\n",
    "# get_target = lambda df: (df['duration'].values, df['event'].values)\n",
    "# y_train = labtrans.fit_transform(*get_target(df_train))\n",
    "# y_val = labtrans.transform(*get_target(df_val))\n",
    "\n",
    "# train = (x_train, y_train)\n",
    "# val = (x_val, y_val)\n",
    "\n",
    "# We don't need to transform the test labels\n",
    "# durations_test, events_test = get_target(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Params\n",
    "input_size = x_train.shape[1]\n",
    "hidden_size = 32\n",
    "output_size = train_trans.bucket_indices\n",
    "# batch_norm = True\n",
    "dropout = 0.1\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "\n",
    "# Convert dtypes\n",
    "dur_idx = torch.Tensor(y_train[0].astype('int'))\n",
    "events = torch.Tensor(y_train[1].astype('int') )\n",
    "\n",
    "dur_idx_val = torch.Tensor(y_val[0].astype('int'))\n",
    "events_val = torch.Tensor(y_val[1].astype('int') )\n",
    "\n",
    "# # Define early stopping parameters\n",
    "# patience = 10\n",
    "# best_loss = float('inf')\n",
    "# counter = 0\n",
    "\n",
    "# # init model\n",
    "# model = MLP(input_size , hidden_size , output_size , dropout_rate = dropout)\n",
    "\n",
    "# # init optim\n",
    "# optimizer = torch.optim.Adam(model.parameters() , lr = learning_rate)\n",
    "\n",
    "# # Create an instance of your custom dataset\n",
    "# dataset = MyDataset(x_train, dur_idx, events)\n",
    "\n",
    "# # Create a data loader for batching and shuffling\n",
    "# batch_size = 256\n",
    "# shuffle = True\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 3.2578, Validation Loss: 3.2015\n",
      "Epoch 1: Training Loss: 3.2343, Validation Loss: 3.1207\n",
      "Epoch 1: Training Loss: 3.0926, Validation Loss: 3.0546\n",
      "Epoch 1: Training Loss: 3.0426, Validation Loss: 2.9939\n",
      "Epoch 1: Training Loss: 3.1451, Validation Loss: 2.9308\n",
      "Epoch 2: Training Loss: 2.8753, Validation Loss: 2.8721\n",
      "Epoch 2: Training Loss: 2.8383, Validation Loss: 2.8115\n",
      "Epoch 2: Training Loss: 2.8314, Validation Loss: 2.7509\n",
      "Epoch 2: Training Loss: 2.7731, Validation Loss: 2.6889\n",
      "Epoch 2: Training Loss: 2.6208, Validation Loss: 2.6235\n",
      "Epoch 3: Training Loss: 2.6679, Validation Loss: 2.5574\n",
      "Epoch 3: Training Loss: 2.4126, Validation Loss: 2.4879\n",
      "Epoch 3: Training Loss: 2.5633, Validation Loss: 2.4175\n",
      "Epoch 3: Training Loss: 2.4713, Validation Loss: 2.3392\n",
      "Epoch 3: Training Loss: 2.3484, Validation Loss: 2.2652\n",
      "Epoch 4: Training Loss: 2.3241, Validation Loss: 2.1894\n",
      "Epoch 4: Training Loss: 2.2049, Validation Loss: 2.1174\n",
      "Epoch 4: Training Loss: 2.1018, Validation Loss: 2.0442\n",
      "Epoch 4: Training Loss: 1.9742, Validation Loss: 1.9637\n",
      "Epoch 4: Training Loss: 2.0837, Validation Loss: 1.8856\n",
      "Epoch 5: Training Loss: 1.8522, Validation Loss: 1.8070\n",
      "Epoch 5: Training Loss: 1.7968, Validation Loss: 1.7364\n",
      "Epoch 5: Training Loss: 1.8200, Validation Loss: 1.6707\n",
      "Epoch 5: Training Loss: 1.8081, Validation Loss: 1.6128\n",
      "Epoch 5: Training Loss: 1.6210, Validation Loss: 1.5627\n",
      "Epoch 6: Training Loss: 1.6067, Validation Loss: 1.5182\n",
      "Epoch 6: Training Loss: 1.5565, Validation Loss: 1.4836\n",
      "Epoch 6: Training Loss: 1.5206, Validation Loss: 1.4580\n",
      "Epoch 6: Training Loss: 1.5467, Validation Loss: 1.4353\n",
      "Epoch 6: Training Loss: 1.4461, Validation Loss: 1.4175\n",
      "Epoch 7: Training Loss: 1.4386, Validation Loss: 1.4026\n",
      "Epoch 7: Training Loss: 1.4687, Validation Loss: 1.3892\n",
      "Epoch 7: Training Loss: 1.4619, Validation Loss: 1.3760\n",
      "Epoch 7: Training Loss: 1.4361, Validation Loss: 1.3674\n",
      "Epoch 7: Training Loss: 1.4334, Validation Loss: 1.3617\n",
      "Epoch 8: Training Loss: 1.4417, Validation Loss: 1.3593\n",
      "Epoch 8: Training Loss: 1.4288, Validation Loss: 1.3588\n",
      "Epoch 8: Training Loss: 1.4017, Validation Loss: 1.3564\n",
      "Epoch 8: Training Loss: 1.4252, Validation Loss: 1.3530\n",
      "Epoch 8: Training Loss: 1.3730, Validation Loss: 1.3514\n",
      "Epoch 9: Training Loss: 1.3323, Validation Loss: 1.3509\n",
      "Epoch 9: Training Loss: 1.4256, Validation Loss: 1.3513\n",
      "Epoch 9: Training Loss: 1.3479, Validation Loss: 1.3530\n",
      "Epoch 9: Training Loss: 1.3426, Validation Loss: 1.3566\n",
      "Epoch 9: Training Loss: 1.5211, Validation Loss: 1.3580\n",
      "Epoch 10: Training Loss: 1.3699, Validation Loss: 1.3606\n",
      "Epoch 10: Training Loss: 1.3232, Validation Loss: 1.3616\n",
      "Epoch 10: Training Loss: 1.5344, Validation Loss: 1.3616\n",
      "Epoch 10: Training Loss: 1.2662, Validation Loss: 1.3617\n",
      "Epoch 10: Training Loss: 1.3239, Validation Loss: 1.3626\n",
      "Early stopping at epoch 10.\n",
      "Early stopping at epoch 11.\n",
      "Early stopping at epoch 12.\n",
      "Early stopping at epoch 13.\n",
      "Early stopping at epoch 14.\n",
      "Early stopping at epoch 15.\n",
      "Early stopping at epoch 16.\n",
      "Early stopping at epoch 17.\n",
      "Early stopping at epoch 18.\n",
      "Early stopping at epoch 19.\n",
      "Early stopping at epoch 20.\n",
      "Early stopping at epoch 21.\n",
      "Early stopping at epoch 22.\n",
      "Early stopping at epoch 23.\n",
      "Early stopping at epoch 24.\n",
      "Early stopping at epoch 25.\n",
      "Early stopping at epoch 26.\n",
      "Early stopping at epoch 27.\n",
      "Early stopping at epoch 28.\n",
      "Early stopping at epoch 29.\n",
      "Early stopping at epoch 30.\n",
      "Early stopping at epoch 31.\n",
      "Early stopping at epoch 32.\n",
      "Early stopping at epoch 33.\n",
      "Early stopping at epoch 34.\n",
      "Early stopping at epoch 35.\n",
      "Early stopping at epoch 36.\n",
      "Early stopping at epoch 37.\n",
      "Early stopping at epoch 38.\n",
      "Early stopping at epoch 39.\n",
      "Early stopping at epoch 40.\n",
      "Early stopping at epoch 41.\n",
      "Early stopping at epoch 42.\n",
      "Early stopping at epoch 43.\n",
      "Early stopping at epoch 44.\n",
      "Early stopping at epoch 45.\n",
      "Early stopping at epoch 46.\n",
      "Early stopping at epoch 47.\n",
      "Early stopping at epoch 48.\n",
      "Early stopping at epoch 49.\n",
      "Early stopping at epoch 50.\n",
      "Early stopping at epoch 51.\n",
      "Early stopping at epoch 52.\n",
      "Early stopping at epoch 53.\n",
      "Early stopping at epoch 54.\n",
      "Early stopping at epoch 55.\n",
      "Early stopping at epoch 56.\n",
      "Early stopping at epoch 57.\n",
      "Early stopping at epoch 58.\n",
      "Early stopping at epoch 59.\n",
      "Early stopping at epoch 60.\n",
      "Early stopping at epoch 61.\n",
      "Early stopping at epoch 62.\n",
      "Early stopping at epoch 63.\n",
      "Early stopping at epoch 64.\n",
      "Early stopping at epoch 65.\n",
      "Early stopping at epoch 66.\n",
      "Early stopping at epoch 67.\n",
      "Early stopping at epoch 68.\n",
      "Early stopping at epoch 69.\n",
      "Early stopping at epoch 70.\n",
      "Early stopping at epoch 71.\n",
      "Early stopping at epoch 72.\n",
      "Early stopping at epoch 73.\n",
      "Early stopping at epoch 74.\n",
      "Early stopping at epoch 75.\n",
      "Early stopping at epoch 76.\n",
      "Early stopping at epoch 77.\n",
      "Early stopping at epoch 78.\n",
      "Early stopping at epoch 79.\n",
      "Early stopping at epoch 80.\n",
      "Early stopping at epoch 81.\n",
      "Early stopping at epoch 82.\n",
      "Early stopping at epoch 83.\n",
      "Early stopping at epoch 84.\n",
      "Early stopping at epoch 85.\n",
      "Early stopping at epoch 86.\n",
      "Early stopping at epoch 87.\n",
      "Early stopping at epoch 88.\n",
      "Early stopping at epoch 89.\n",
      "Early stopping at epoch 90.\n",
      "Early stopping at epoch 91.\n",
      "Early stopping at epoch 92.\n",
      "Early stopping at epoch 93.\n",
      "Early stopping at epoch 94.\n",
      "Early stopping at epoch 95.\n",
      "Early stopping at epoch 96.\n",
      "Early stopping at epoch 97.\n",
      "Early stopping at epoch 98.\n",
      "Early stopping at epoch 99.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=9, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.1, inplace=False)\n",
       "    (8): Linear(in_features=32, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_id , (cov , dur , eve) in enumerate(dataloader):\n",
    "        # Prime for training\n",
    "        model.train()\n",
    "        \n",
    "        # forward pass\n",
    "        phi_train = model(torch.Tensor(cov))\n",
    "\n",
    "        # get loss\n",
    "        loss = nll_logistic_hazard(phi_train , dur , eve)\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Prime for evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            phi_val = model(torch.Tensor(x_val))\n",
    "            val_loss = nll_logistic_hazard(phi_val, dur_idx_val , events_val)\n",
    "        \n",
    "        # Check if validation loss has improved\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        # Check if early stopping condition is met\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Training Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}\") \n",
    "\n",
    "# turn training off\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "hazards = torch.sigmoid(model(torch.Tensor(x_test)))\n",
    "survival = torch.cumprod(1 - hazards , dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hazards[:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for surv in survival[:3]:\n",
    "    plt.step(labtrans.cuts , surv.detach().numpy());"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
